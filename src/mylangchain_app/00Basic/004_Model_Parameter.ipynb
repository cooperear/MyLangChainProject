{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_y\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ëª¨ë¸ í´ë˜ìŠ¤ ìœ í˜•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-instruct\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: gsk_yRNq********************************************B8mo. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m llm = OpenAI()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(llm.model_name)  \u001b[38;5;66;03m# ê¸°ë³¸ ëª¨ë¸ í™•ì¸\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mí•œêµ­ì˜ ëŒ€í‘œì ì¸ ê´€ê´‘ì§€ 3êµ°ë°ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(result))\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.13\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:390\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    381\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    386\u001b[39m     **kwargs: Any,\n\u001b[32m    387\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    388\u001b[39m     config = ensure_config(config)\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    401\u001b[39m         .text\n\u001b[32m    402\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.13\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:789\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     **kwargs: Any,\n\u001b[32m    787\u001b[39m ) -> LLMResult:\n\u001b[32m    788\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.13\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1000\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    986\u001b[39m     run_managers = [\n\u001b[32m    987\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    988\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    998\u001b[39m         )\n\u001b[32m    999\u001b[39m     ]\n\u001b[32m-> \u001b[39m\u001b[32m1000\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1008\u001b[39m     run_managers = [\n\u001b[32m   1009\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m   1010\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1017\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m   1018\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.13\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:815\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    805\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    806\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m     **kwargs: Any,\n\u001b[32m    812\u001b[39m ) -> LLMResult:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    814\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    819\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    823\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    824\u001b[39m         )\n\u001b[32m    825\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    826\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.13\\Lib\\site-packages\\langchain_openai\\llms\\base.py:446\u001b[39m, in \u001b[36mBaseOpenAI._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    430\u001b[39m     choices.append(\n\u001b[32m    431\u001b[39m         {\n\u001b[32m    432\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: generation.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    443\u001b[39m         }\n\u001b[32m    444\u001b[39m     )\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    448\u001b[39m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[32m    449\u001b[39m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[32m    450\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.13\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.13\\Lib\\site-packages\\openai\\resources\\completions.py:541\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    514\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    540\u001b[39m ) -> Completion | Stream[Completion]:\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_of\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mecho\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msuffix\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.13\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\winds\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mylangchain-app-SBe-Yh6W-py3.13\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: gsk_yRNq********************************************B8mo. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# modelì„ ì§€ì •í•˜ì§€ ì•Šì•„ë„ ì‹¤í–‰ë˜ëŠ” ì´ìœ ëŠ” LangChainì´ ë‚´ë¶€ì ìœ¼ë¡œ ê¸°ë³¸ê°’ì„ ì„¤ì •í•¨\n",
    "llm = OpenAI()\n",
    "print(llm.model_name)  # ê¸°ë³¸ ëª¨ë¸ í™•ì¸\n",
    "\n",
    "result = llm.invoke(\"í•œêµ­ì˜ ëŒ€í‘œì ì¸ ê´€ê´‘ì§€ 3êµ°ë°ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\")\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ì•ˆë…•í•˜ì„¸ìš”! í•œêµ­ì„ ëŒ€í‘œí•˜ëŠ” 3ê³³ì„ ê¼½ìë©´, K-ë¬¸í™”Â·ì—­ì‚¬Â·ìì—°ì„ ëª¨ë‘ ì¶©ì¡±ì‹œí‚¤ëŠ” â€œë°˜ë“œëŸ¬ ê°€ì•¼ í•  ëª…ì†Œâ€ë“¤ì…ë‹ˆë‹¤.\n",
      "\n",
      "1. ê²½ë³µê¶ & ë¶ì´Œí•œì˜¥ë§ˆì„(ì„œìš¸)  \n",
      "   â€¢çœ‹é»: ì¡°ì„  ì™•ê¶ì˜ ì •ìˆ˜, ë§¤ì‹œê°„ ìˆ˜ë¬¸ì¥ êµëŒ€ì‹, ê¶ê¶ ë’¤ë¡œ ë³´ì´ëŠ” ì‚°(ë¶ì•…ì‚°) í’ê²½  \n",
      "   â€¢ ê¿€íŒ: í•´ì„¤ì´ ìˆëŠ” 1ì‹œê°„ ë‹¨ì²´ ê´€ëŒ(ë¬´ë£Œ) ì‹ ì²­í•˜ë©´ ì—­ì‚¬ ì´í•´ë„ â†‘, ì´í›„ ë¶ì´Œ 8ê²½ ì‚°ì±… + í•œì˜¥ ì¹´í˜ íœ´ì‹  \n",
      "   â€¢ êµí†µ: ì§€í•˜ì²‘ 3í˜¸ì„  ê²½ë³µê¶ì—­ 5ë²ˆ ì¶œêµ¬ ë„ë³´ 5ë¶„\n",
      "\n",
      "2. ë¶€ì‚° í•´ìš´ëŒ€ & ê°ì² ë¬¸í™”ë§ˆì„(ë¶€ì‚°)  \n",
      "   â€¢çœ‹é»: ëŒ€í‘œ í•´ë³€+ì•¼ê²½, ì´ì–´ì§€ëŠ” ë‹¬ë§ì´ê³¨ëª©Â·ë²¡ì‚¬, íŠ¸ë¨í”„ì¹´í˜, ì ˆ cliffs-walk(ë°”ë‹·ê¸¸ ì‚°ì±…)  \n",
      "   â€¢ ê¿€íŒ: í•´ìš´ëŒ€ ë‹¬ë§ì´ì „ë§ëŒ€ì—ì„œ ì„ì–‘, ì´í›„ ë¯¸í¬ì² ê¸¸(ì˜›ê¸°ì°»ê¸¸)ë¡œ ì´ì–´ì§€ëŠ” 30ë¶„ ì½”ìŠ¤  \n",
      "   â€¢ êµí†µ: ë¶€ì‚°ì—­â†’ì§€í•˜ì²‘ 2í˜¸ì„  í•´ìš´ëŒ€ì—­ 30ë¶„\n",
      "\n",
      "3. ì œì£¼ë„ ì„±ì‚°ì¼ì¶œë´ & ìš°ë„(ì œì£¼)  \n",
      "   â€¢çœ‹é»: ìœ ë„¤ìŠ¤ì½” ì„¸ê³„ìì—°ìœ ì‚°, ì¼ì¶œ ëª…ì†Œ, íŠ¸ë ˆí‚¹(ì•½ 1ì‹œê°„), ìš°ë„ ì „ê¸°ë°”ì´í¬ ì¼ì£¼(2~3ì‹œê°„)  \n",
      "   â€¢ ê¿€íŒ: ë´„Â·ê°€ì„ì—” ë¯¼ë“¤ë ˆÂ·ìœ ì±„ê½ƒ, ì—¬ë¦„ì—” ìŠ¤ë…¸í´ë§, ê²¨ìš¸ì—” ì„±ì‚° ì¼ì¶œì´ í•«í”Œ; ìš°ë„ í•´ë…€ì´Œ í‘ë¼ì§€Â·ë•…ì½©ì•„ì´ìŠ¤í¬ë¦¼ í•„ìˆ˜  \n",
      "   â€¢ êµí†µ: ì œì£¼ê³µí•­â†’ì„±ì‚°ì¼ì¶œë´ 1ì‹œê°„ 20ë¶„(ë²„ìŠ¤ 111/112), ìš°ë„ì„ ì°©ì¥â†’ìš°ë„ 15ë¶„ ë°°\n",
      "\n",
      "ì´ 3ê³³ë§Œ ì˜ ì§œë©´ ì„œìš¸(ë„ì‹¬Â·ì—­ì‚¬) â†” ë¶€ì‚°(ë°”ë‹¤Â·ì•¼ê²½) â†” ì œì£¼(ìì—°Â·ì„¬)ë¥¼ ì´ì–´ í•œêµ­ ì—¬í–‰ì˜ ì •ìˆ˜ë¥¼ í•œ ë²ˆì— ê²½í—˜í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chat = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ì´ ì‹œìŠ¤í…œì€ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "chain = chat_prompt | chat\n",
    "result = chain.invoke({\"user_input\": \"ì•ˆë…•í•˜ì„¸ìš”? í•œêµ­ì˜ ëŒ€í‘œì ì¸ ê´€ê´‘ì§€ 3êµ°ë°ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\"})\n",
    "\n",
    "print(type(result))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "##### Temperature íš¨ê³¼\n",
    "* 0.2 (ë‚®ì€ ê°’): ë” ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ì¼ë°˜ì ì¸ ì´ì•¼ê¸°\n",
    "* 1.2 (ë†’ì€ ê°’): ìƒˆë¡œìš´ ìš”ì†Œê°€ ë“±ì¥í•˜ë©°, ë” ì°½ì˜ì ì´ê³  í¥ë¯¸ë¡œìš´ ì´ì•¼ê¸° ìƒì„±\n",
    "\n",
    "##### Presence Penalty íš¨ê³¼\n",
    "* 0.0 (ë‚®ì€ ê°’): ê¸°ì¡´ì— ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì™€ êµ¬ì¡° ìœ ì§€\n",
    "* 1.5 (ë†’ì€ ê°’): ìƒˆë¡œìš´ í‘œí˜„ê³¼ ë…ì°½ì ì¸ ì•„ì´ë””ì–´ ë“±ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " CONSERVATIVE ì„¤ì • (ì˜¨ë„:0.1, presence:-0.5, frequency:-0.3, top_p:0.3)\n",
      "  â†’ ì „í†µì ì´ê³  í•™ìˆ ì ì¸ ì² í•™ì  ì ‘ê·¼\n",
      "============================================================\n",
      "ì‹œê°„ì€  \n",
      "â€œìš°ì£¼ê°€ ìì‹ ì˜ ëª¸ì†ì—ì„œ ëŠë¼ëŠ” â€˜ë–¨ë¦¼â€™ì´ ì¸ê°„ì—ê²Œ ë²ˆì—­ëœ ê²ƒâ€ì´ë‹¤.\n",
      "\n",
      "ìš°ë¦¬ê°€ í”íˆ â€˜1ì´ˆâ€™ í˜¹ì€ â€˜1ë…„â€™ì´ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì€, ì‚¬ì‹¤ ìš°ì£¼ ì „ì²´ê°€ í•œ ë²ˆ â€˜ìˆ¨ì‰¬ëŠ”â€™ ë™ì•ˆ ìì‹ ì˜ í”¼ë¶€ì— ì¼ì–´ë‚œ ë¯¸ì„¸í•œ ë–¨ë¦¼ì„ ì¸ê°„ì´ë¼ëŠ” ì‘ì€ ì‹ ê²½ì„¸í¬ê°€ ì „ê¸°ë¡œ ë°”ê¾¼ ë’¤, ê·¸ íšŸìˆ˜ë¥¼ ì„¸ê³  ì´ë¦„ ë¶™ì¸ ì°©ì‹œì¼ ë¿ì´ë‹¤.  \n",
      "ì¦‰, ì‹œê°„ì€ ì¡´ì¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ â€˜ì¡´ì¬ê°€ ëŠë¼ëŠ” ë°©ì‹â€™ì´ë‹¤.  \n",
      "ìš°ì£¼ê°€ ìˆ¨ì„ ë©ˆì¶”ë©´ ì‹œê°„ë„ ë©ˆì¶”ê³ , ìš°\n",
      "\n",
      "============================================================\n",
      " CREATIVE ì„¤ì • (ì˜¨ë„:1.5, presence:1.5, frequency:1.2, top_p:0.95)\n",
      "  â†’ ì¶”ìƒì ì´ê³  ë…ì°½ì ì¸ ì² í•™ì  ì‚¬ê³ \n",
      "============================================================\n",
      "ì‹œê°„ì€, ìš°ë¦¬ê°€ â€˜ìŠê³  ì§€ë‚¸ ì¶”ì–µâ€™ì´ ì„œë¡œë¥¼ ë§ˆì£¼ì¹˜ëŠ” ìˆœê°„, ë¹›ë‚˜ëŠ” ë¬´ìˆ˜í•œ ë¯¸ì„¸ ì…ìì²˜ëŸ¼ í”¼ì–´ì˜¤ë¥´ëŠ” â€˜ìŠí˜€ì§ì˜ ë“±ì”ë¶ˆâ€™ì´ë‹¤.\n",
      "\n",
      "ìš°ë¦¬ê°€ ì„œë¡œ â€œì–˜ê¸° ì¢€ ë‚˜ëˆ„ìâ€í•˜ë©° ê°™ì€ í…Œì´ë¸”ì— ì•‰ëŠ” ì°°ë‚˜, ì‹¤ì€ ê°ìê°€ ë¯¸ì²˜ ì²´ë¥˜í•˜ì§€ ëª»í•œ â€˜ê¸°íšŒì˜ ëŒ€ê¸°ì„â€™ì´ ê³µê°„ í•˜ë‚˜ ì„ ë°˜ì²˜ëŸ¼ ë‘¥ ë–  ìˆë‹¤. í…Œì´ë¸” ì•„ë˜ì—ì„  ìˆ¨ì€ â€˜ëª»ë‹¤í•œ ì§ˆë¬¸â€™ë“¤ì´ ì•„ë¼ë¹„ì•ˆë°”ëŒì²˜ëŸ¼ ì™”ë‹¤ ê°”ë‹¤ í•œë‹¤. ê·¸ë“¤ì€ í•œ ì¤Œ í’€ê½ƒ ì”¨ì•—ì´ìš”, ì´ í–‰ìœ„ ê·¸ ìì²´ì— â€˜ì§€ê¸ˆâ€™ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” ì‹¹ì´ íŠ¼ë‹¤. ìš°ë¦¬ëŠ” ëŠ˜ â€œê·¸ë¬êµ¬ë‚˜â€í•˜ë©° ê¸°ë…ì‚¬ì§„ ëŒ€ì‹  ê·¸ ì”¨ì•—ë§Œ ì‚´í¬í•œë‹¤. ì§€ê¸ˆì„ ê¸°ë¡í•˜ëŠ” ìˆœê°„, ê¸°ë¡ì€ íœ´ì‹ê¸°ì— ë‚˜ë‰˜ê³  ë‚˜ë¨¸ì§€ëŠ” í‹ˆ, ë¶„ì ˆ, ìš°ì£¼ì— í‘¹ ì ê¹¨ì–´ë‚˜ ë¦¬ë³¸ ë¬¶ë“¯ì´ ëª¨ì—¬ ìˆë‹¤.\n",
      "\n",
      "ì´ ì”¨ì•— ë­‰ì¹˜ê°€ â€˜ì¶”ì–µâ€™ì´ëƒê³ ? ê·¸ê±´ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤. ë‚´ëŸ¬í‹°ë¸Œ(ì‚¬ë¬¼\n",
      "\n",
      "============================================================\n",
      " íŒŒë¼ë¯¸í„° ì°¨ì´ì  ìš”ì•½:\n",
      "   â€¢ Temperature: 0.1 vs 1.5 (15ë°° ì°¨ì´)\n",
      "   â€¢ Presence Penalty: -0.5 vs 1.5 (ê¸°ì¡´ íŒ¨í„´ ì„ í˜¸ vs ìƒˆë¡œìš´ ê°œë… ê°•ìš”)\n",
      "   â€¢ Frequency Penalty: -0.3 vs 1.2 (ë°˜ë³µ í—ˆìš© vs ê°•í•œ ë‹¤ì–‘ì„±)\n",
      "   â€¢ Top-p: 0.3 vs 0.95 (ì œí•œì  vs ê±°ì˜ ë¬´ì œí•œ í† í° ì„ íƒ)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ë³´ìˆ˜ì ì¸ ì„¤ì • (ì¼ê´€ë˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë‹µë³€)\n",
    "llm_conservative = ChatOpenAI(    \n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"gpt-3.5-turbo-0125\",\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",    \n",
    "    temperature=0.1,  # ë§¤ìš° ë‚®ì€ ì˜¨ë„ - ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ì¼ê´€ëœ ì¶œë ¥\n",
    "    presence_penalty=-0.5,  # ê¸°ì¡´ íŒ¨í„´ê³¼ í‘œí˜„ì„ ê°•í•˜ê²Œ ì„ í˜¸\n",
    "    frequency_penalty=-0.3,  # ë°˜ë³µì  í‘œí˜„ í—ˆìš©, ì¼ê´€ì„± ì¦ëŒ€\n",
    "    max_tokens=200,  # ì ë‹¹í•œ ê¸¸ì´\n",
    "    top_p=0.3  # ë§¤ìš° ì œí•œì ì¸ í† í° ì„ íƒ - ì•ˆì „í•˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë‹¨ì–´ë§Œ\n",
    ")\n",
    "\n",
    "# ì°½ì˜ì ì¸ ì„¤ì • (ë…ì°½ì ì´ê³  ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ë‹µë³€)\n",
    "llm_creative = ChatOpenAI(\n",
    "    #model=\"gpt-3.5-turbo-0125\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\", \n",
    "    temperature=1.5,  # ë§¤ìš° ë†’ì€ ì˜¨ë„ - ì°½ì˜ì ì´ê³  ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ì¶œë ¥\n",
    "    presence_penalty=1.5,  # ìƒˆë¡œìš´ ì£¼ì œì™€ ê°œë…ì„ ê°•í•˜ê²Œ ìœ ë„\n",
    "    frequency_penalty=1.2,  # ë°˜ë³µì„ ê°•í•˜ê²Œ ì–µì œí•˜ì—¬ ë§¤ìš° ë‹¤ì–‘í•œ í‘œí˜„ ìƒì„±\n",
    "    max_tokens=350,  # ë” ê¸´ ì°½ì˜ì  ì„œìˆ  í—ˆìš©\n",
    "    top_p=0.95  # ê±°ì˜ ëª¨ë“  í† í° í›„ë³´ì—ì„œ ì„ íƒ - ìµœëŒ€ ì°½ì˜ì„±\n",
    ")\n",
    "\n",
    "# ì§ˆë¬¸ ì„¤ì •: íŒŒë¼ë¯¸í„° ì°¨ì´ê°€ ê·¹ëª…í•˜ê²Œ ë“œëŸ¬ë‚˜ëŠ” ì¶”ìƒì /ì² í•™ì  ì§ˆë¬¸\n",
    "question = \"\"\"\n",
    "\"ì‹œê°„\"ì´ë¼ëŠ” ê°œë…ì„ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì •ì˜í•œë‹¤ë©´ ì–´ë–»ê²Œ í‘œí˜„í•  ìˆ˜ ìˆì„ê¹Œìš”?\n",
    "ì¼ìƒì ì¸ ì‹œê³„ë‚˜ ë‹¬ë ¥ì˜ ê´€ì ì´ ì•„ë‹Œ, ì™„ì „íˆ ìƒˆë¡œìš´ ì‹œê°ì—ì„œ ì‹œê°„ì„ ë°”ë¼ë³´ê³  ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "ê¸°ì¡´ì˜ ê³ ì •ê´€ë…ì„ ë²—ì–´ë‚˜ì„œ ììœ ë¡­ê²Œ ì‚¬ê³ í•´ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" CONSERVATIVE ì„¤ì • (ì˜¨ë„:0.1, presence:-0.5, frequency:-0.3, top_p:0.3)\")\n",
    "print(\"  â†’ ì „í†µì ì´ê³  í•™ìˆ ì ì¸ ì² í•™ì  ì ‘ê·¼\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë³´ìˆ˜ì  ëª¨ë¸ í˜¸ì¶œ\n",
    "response_conservative = llm_conservative.invoke(question)\n",
    "print(response_conservative.content)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" CREATIVE ì„¤ì • (ì˜¨ë„:1.5, presence:1.5, frequency:1.2, top_p:0.95)\")\n",
    "print(\"  â†’ ì¶”ìƒì ì´ê³  ë…ì°½ì ì¸ ì² í•™ì  ì‚¬ê³ \")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ì°½ì˜ì  ëª¨ë¸ í˜¸ì¶œ\n",
    "response_creative = llm_creative.invoke(question)\n",
    "print(response_creative.content)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" íŒŒë¼ë¯¸í„° ì°¨ì´ì  ìš”ì•½:\")\n",
    "print(\"   â€¢ Temperature: 0.1 vs 1.5 (15ë°° ì°¨ì´)\")\n",
    "print(\"   â€¢ Presence Penalty: -0.5 vs 1.5 (ê¸°ì¡´ íŒ¨í„´ ì„ í˜¸ vs ìƒˆë¡œìš´ ê°œë… ê°•ìš”)\")\n",
    "print(\"   â€¢ Frequency Penalty: -0.3 vs 1.2 (ë°˜ë³µ í—ˆìš© vs ê°•í•œ ë‹¤ì–‘ì„±)\")\n",
    "print(\"   â€¢ Top-p: 0.3 vs 0.95 (ì œí•œì  vs ê±°ì˜ ë¬´ì œí•œ í† í° ì„ íƒ)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameter\n",
    "* Temperature (ì˜¨ë„): ì´ íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. \n",
    "    * ë‚®ì€ ì˜¨ë„ëŠ” ëª¨ë¸ì´ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ê²Œ í•˜ì—¬ ì•ˆì •ì ì¸ ê²°ê³¼ë¥¼ ë‚´ê³ , ë†’ì€ ì˜¨ë„ëŠ” í™•ë¥ ì´ ë‚®ì€ ë‹¨ì–´ê¹Œì§€ ì„ íƒì§€ì— í¬í•¨ì‹œì¼œ ë” ì°½ì˜ì ì´ê³  ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "* Presence Penalty & Frequency Penalty: ë‘ íŒŒë¼ë¯¸í„° ëª¨ë‘ í† í°(ë‹¨ì–´)ì˜ ë°˜ë³µì„±ì„ ì œì–´í•©ë‹ˆë‹¤.\n",
    "    * Presence PenaltyëŠ” ì´ì „ì— ë‚˜ì˜¨ ë‹¨ì–´ë‚˜ ì£¼ì œì— 'í˜ë„í‹°'ë¥¼ ì£¼ì–´ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ê²Œ í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ ìƒˆë¡œìš´ ë‚´ìš©ì„ ë” ìì£¼ ì‹œë„í•©ë‹ˆë‹¤.\n",
    "    * Frequency PenaltyëŠ” ë‹¨ì–´ê°€ ë“±ì¥í•œ ë¹ˆë„ì— ë¹„ë¡€í•˜ì—¬ 'í˜ë„í‹°'ë¥¼ ì¤ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ íŠ¹ì • ë‹¨ì–´ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê°•í•˜ê²Œ ì–µì œí•˜ì—¬ ë” ë‹¤ì–‘í•œ ì–´íœ˜ë¥¼ ì‚¬ìš©í•˜ê²Œ í•©ë‹ˆë‹¤. \n",
    "* Top_p: ì´ íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒ í† í°ì„ ì„ íƒí•  ë•Œ ê³ ë ¤í•  ë‹¨ì–´ì˜ ë²”ìœ„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤. Top_pê°€ 0.3ì´ë©´ í™•ë¥ ì´ ë†’ì€ ìƒìœ„ 30%ì˜ ë‹¨ì–´ ì¤‘ì—ì„œë§Œ ì„ íƒí•˜ê³ , 0.95ë©´ í™•ë¥ ì´ ë‚®ì€ ë‹¨ì–´ë“¤ê¹Œì§€ í¬í•¨í•˜ì—¬ ê±°ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤. Top_pê°€ ë†’ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ì„ íƒì§€ê°€ ë„“ì–´ì ¸ ë” ììœ ë¡œìš´ ì‘ë‹µì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ë³´ìˆ˜ì ì¸ ì„¤ì •: ë…¼ë¦¬ì ì´ê³  ì¼ê´€ëœ ì„œìˆ \n",
    "llm_conservative = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY, \n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\", \n",
    "    temperature=0.1,         # ë‚®ì€ ê°’(0.1): 'í™•ë¥ ì˜ í­'ì„ ì¢í˜€ ê°€ì¥ ì•ˆì „í•˜ê³  í”í•œ ë‹¨ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "                             # â†’ ê²°ê³¼ê°€ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ë°˜ë³µì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì¹˜ ì •í•´ì§„ êµê³¼ì„œì²˜ëŸ¼.\n",
    "    presence_penalty=-0.5,   # ìŒìˆ˜ ê°’(-0.5): ì´ë¯¸ ë“±ì¥í–ˆë˜ ì£¼ì œë‚˜ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "                             # â†’ ê¸°ì¡´ íŒ¨í„´ì„ ë²—ì–´ë‚˜ì§€ ì•Šê³  ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "    frequency_penalty=-0.3,  # ìŒìˆ˜ ê°’(-0.3): ìì£¼ ë“±ì¥í•œ ë‹¨ì–´ì— ëŒ€í•œ 'ë²Œì¹™'ì„ ì¤„ì—¬, ë™ì¼í•œ í‘œí˜„ì˜ ë°˜ë³µì„ í—ˆìš©í•©ë‹ˆë‹¤.\n",
    "                             # â†’ ì‘ë‹µì˜ ì¼ê´€ì„±ì´ ë†’ì•„ì§€ê³ , í•µì‹¬ ì£¼ì œì—ì„œ ë²—ì–´ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "    max_tokens=200,          # ì‘ë‹µì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì œí•œí•©ë‹ˆë‹¤. ë³´ìˆ˜ì ì¸ ë‹µë³€ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "    top_p=0.3                # ë‚®ì€ ê°’(0.3): í™•ë¥ ì´ ë†’ì€ ìƒìœ„ 30% í† í°(ë‹¨ì–´)ë§Œ ê³ ë ¤í•©ë‹ˆë‹¤.\n",
    "                             # â†’ ëª¨ë¸ì˜ 'ì„ íƒì§€'ë¥¼ ê·¹ë„ë¡œ ì œí•œí•˜ì—¬ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë‹¨ì–´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    ")\n",
    "\n",
    "# ì°½ì˜ì ì¸ ì„¤ì •: ë…ì°½ì ì´ê³  í’ë¶€í•œ ë¹„ìœ ì™€ ì€ìœ \n",
    "llm_creative = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=1.5,         # ë†’ì€ ê°’(1.5): 'í™•ë¥ ì˜ í­'ì„ ë„“í˜€ í”ì¹˜ ì•Šì€ ë‹¨ì–´ê¹Œì§€ ì„ íƒí•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "                             # â†’ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê³  ë…ì°½ì ì¸ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ˆì¹˜ ì¦‰í¥ì ì¸ ì‹œì¸ì²˜ëŸ¼ìš”.\n",
    "    presence_penalty=1.5,    # ì–‘ìˆ˜ ê°’(1.5): ì´ë¯¸ ì‚¬ìš©ëœ ì£¼ì œë‚˜ ê°œë…ì— 'ë²Œì¹™'ì„ ì£¼ì–´ ìƒˆë¡œìš´ ê²ƒì„ ì‹œë„í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "                             # â†’ ë…ì°½ì ì¸ ì•„ì´ë””ì–´ë‚˜ ê´€ì ì„ ê°•í•˜ê²Œ ìœ ë„í•©ë‹ˆë‹¤.\n",
    "    frequency_penalty=1.2,   # ì–‘ìˆ˜ ê°’(1.2): ìì£¼ ë“±ì¥í•œ ë‹¨ì–´ì— 'ë²Œì¹™'ì„ ì£¼ì–´ ë°˜ë³µì„ ê°•ë ¥í•˜ê²Œ ì–µì œí•©ë‹ˆë‹¤.\n",
    "                             # â†’ í‘œí˜„ì˜ ë‹¤ì–‘ì„±ì„ ê·¹ëŒ€í™”í•˜ê³  ë¬¸ì¥ì´ í’ë¶€í•´ì§‘ë‹ˆë‹¤.\n",
    "    max_tokens=350,          # ë” ê¸´ ì°½ì˜ì  ì„œìˆ ì„ í—ˆìš©í•˜ê¸° ìœ„í•´ ìµœëŒ€ ê¸¸ì´ë¥¼ ëŠ˜ë¦½ë‹ˆë‹¤.\n",
    "    top_p=0.95               # ë†’ì€ ê°’(0.95): í™•ë¥ ì´ ë‚®ì€ 95%ì˜ ëª¨ë“  í† í°ì„ ê³ ë ¤í•©ë‹ˆë‹¤.\n",
    "                             # â†’ ëª¨ë¸ì˜ 'ì„ íƒì§€'ë¥¼ ê±°ì˜ ë¬´ì œí•œìœ¼ë¡œ í™•ì¥í•˜ì—¬ ìµœëŒ€ì˜ ì°½ì˜ì„±ì„ ë°œíœ˜í•©ë‹ˆë‹¤.\n",
    ")\n",
    "\n",
    "# ì§ˆë¬¸ì„ ChatPromptTemplateìœ¼ë¡œ ì •ì˜í•˜ì—¬ ì—­í• ê³¼ ì§ˆë¬¸ì„ ëª…í™•íˆ í•¨\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ì‘ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ í’ë¶€í•œ ë¹„ìœ ì™€ ì€ìœ ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œìˆ í•´ì£¼ì„¸ìš”.\"),\n",
    "    (\"human\", \"\"\"\n",
    "    \"ë„ì‹œì˜ ì†ŒìŒ\"ì„ ìƒëª…ì²´ë‚˜ ìì—° í˜„ìƒì— ë¹—ëŒ€ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "    ì¼ìƒì ì¸ ì†Œë¦¬ê°€ ì•„ë‹Œ, ê·¸ ì†Œë¦¬ê°€ ê°€ì§„ ìƒëª…ë ¥ê³¼ ê°ì •ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”.\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "# íŒŒì„œ ì„¤ì •\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±: Prompt -> Model -> Parser\n",
    "chain_conservative = prompt | llm_conservative | parser\n",
    "chain_creative = prompt | llm_creative | parser\n",
    "\n",
    "# ëª¨ë¸ í˜¸ì¶œ ë° ê²°ê³¼ ì¶œë ¥\n",
    "print(\"=\" * 60)\n",
    "print(\" ë³´ìˆ˜ì  ì„¤ì • (ë…¼ë¦¬ì , ì¼ê´€ì )\")\n",
    "print(\" â†’ ì¼ìƒì ì´ê³  ì¼ë°˜ì ì¸ ë¹„ìœ ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ ë‹µë³€\")\n",
    "print(\"=\" * 60)\n",
    "response_conservative = chain_conservative.invoke({})\n",
    "print(response_conservative)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" ì°½ì˜ì  ì„¤ì • (í’ë¶€í•œ ë¹„ìœ ì™€ ì€ìœ )\")\n",
    "print(\" â†’ ì˜ˆìƒì¹˜ ëª»í•œ ë…ì°½ì ì¸ ë¹„ìœ ì™€ ê°ê°ì ì¸ í‘œí˜„ ì‚¬ìš©\")\n",
    "print(\"=\" * 60)\n",
    "response_creative = chain_creative.invoke({})\n",
    "print(response_creative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ì„ ë‹´ì€ í…œí”Œë¦¿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ í’ë¶€í•œ ë¹„ìœ ì™€ ì€ìœ ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œìˆ í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹µë³€ì˜ ê´€ì ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"),\n",
    "    (\"human\", \"\"\"\n",
    "    \"ë¹¨ê°„ ì¥ë¯¸\"ë¥¼ ë‘ ê°€ì§€ ê´€ì ì—ì„œ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”.\n",
    "    1. **ê°ê´€ì  ê´€ì :** ì¥ë¯¸ì˜ ë¬¼ë¦¬ì  íŠ¹ì„±ê³¼ ìƒì§•ì  ì˜ë¯¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "    2. **ë¬¸í•™ì  ê´€ì :** ì¥ë¯¸ë¥¼ ì‚¬ëŒì´ë‚˜ ê°ì •ì— ë¹—ëŒ€ì–´ ììœ ë¡­ê³  ì‹œì ìœ¼ë¡œ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”.\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "# íŒŒì„œ ì„¤ì •\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±: Prompt -> Model -> Parser\n",
    "chain_conservative = prompt | llm_conservative | parser\n",
    "chain_creative = prompt | llm_creative | parser\n",
    "\n",
    "# ëª¨ë¸ í˜¸ì¶œ ë° ê²°ê³¼ ì¶œë ¥\n",
    "print(\"=\" * 60)\n",
    "print(\" ë³´ìˆ˜ì  ì„¤ì • (ê°ê´€ì , ì‚¬ì‹¤ ê¸°ë°˜)\")\n",
    "print(\" â†’ ê³¼í•™ì ì´ê³  ë…¼ë¦¬ì ì¸ ì¥ë¯¸ì˜ íŠ¹ì„± ì„¤ëª…\")\n",
    "print(\"=\" * 60)\n",
    "response_conservative = chain_conservative.invoke({})\n",
    "print(response_conservative)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" ì°½ì˜ì  ì„¤ì • (ë¬¸í•™ì , ìƒìƒ ê¸°ë°˜)\")\n",
    "print(\" â†’ ì€ìœ ì™€ ë¹„ìœ ë¥¼ í™œìš©í•œ ì¥ë¯¸ì˜ ê°ì„±ì  ë¬˜ì‚¬\")\n",
    "print(\"=\" * 60)\n",
    "response_creative = chain_creative.invoke({})\n",
    "print(response_creative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelì˜ ì¢…ë¥˜ì— ë”°ë¼ ê²°ê³¼ ê°’ì´ ë‹¤ë¦„\n",
    "* gpt-4o vs gpt-3.5-turbo-0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ë³€ê²½ëœ ì„¤ì • (ë” ì°½ì˜ì ì¸ ë‹µë³€, ìƒˆë¡œìš´ ì•„ì´ë””ì–´ ìœ ë„)\n",
    "#llm_after = ChatOpenAI(model=\"gpt-4o\", temperature=1.0, presence_penalty=1.5)\n",
    "#llm_after = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=1.0, presence_penalty=1.5)\n",
    "llm_after = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=1.0,\n",
    "    presence_penalty=1.5\n",
    ")\n",
    "\n",
    "# ì§ˆë¬¸ ì„¤ì •\n",
    "question = \"í•œêµ­ì—ì„œ ê°€ë³¼ ë§Œí•œ ì—¬í–‰ì§€ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# ëª¨ë¸ í˜¸ì¶œ\n",
    "response_after = llm_after.invoke(question)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n-------------------------\\n\")\n",
    "print(\" After (ì°½ì˜ì ì¸ ì‘ë‹µ, ìƒˆë¡œìš´ ì•„ì´ë””ì–´ í¬í•¨)\")\n",
    "print(response_after.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-1) ëª¨ë¸ì— ì§ì ‘ íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬ (ëª¨ë¸ ìƒì„± ì‹œì )\n",
    "##### Before / After íŒŒë¼ë¯¸í„° ì°¨ì´\n",
    "* temperature: Before(0.7) â†’ ë‹¤ì–‘ì„± ë†’ì€ ì¶”ì²œ / After(0.3) â†’ ì •í™•í•œ ì¼ì • ì œê³µ\n",
    "* max_tokens: Before(300) â†’ ê°„ëµí•œ ì‘ë‹µ / After(800) â†’ ì„¸ë¶€ ì¼ì • í¬í•¨\n",
    "* frequency_penalty: Before(0.5) â†’ ë‹¨ì–´ ë°˜ë³µ ì–µì œ / After(0.2) â†’ ì¢€ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ\n",
    "* presence_penalty: Before(0.5) â†’ ë‹¤ì–‘í•œ ì¥ì†Œ ì¶”ì²œ / After(0.3) â†’ ìƒˆë¡œìš´ ì •ë³´ í¬í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Before] ëª¨ë¸ ê²°ê³¼\n",
      "# ğŸ 3ë°• 4ì¼ ê°€ì¡±ì—¬í–‰, 'ëŠë¦¼ì˜ ë¯¸í•™' ì½”ìŠ¤\n",
      "\n",
      "ê°€ì¡± ë‹¨ìœ„ë¼ë©´ ì•„ì´ ì–´ë¥¸ í•  ê²ƒ ì—†ì´ \"ì›€ì§ì´ëŠ” ì‹œê°„ ìµœì†Œí™”Â·ì²´ë¥˜ ì‹œê°„ ìµœëŒ€í™”\"ê°€ ìµœê³ ì˜ ëª¨í† ì…ë‹ˆë‹¤.  \n",
      "ì´ ì½”ìŠ¤ëŠ” 'ì°¨ë¡œ 1~2ì‹œê°„ ë‚´ ê±°ë¦¬'ë§Œ ì›€ì§ì´ë©°, 1ê³³ì— í•˜ë£¨ ì´ìƒ ë¨¸ë¬¼ë©° ì‰¬ê³  ì¦ê¸°ëŠ” ì—¬ìœ í˜• ì¼ì •ì…ë‹ˆë‹¤.  \n",
      "ì•„ì´ì™€ ì–´ë¥¸ ëª¨ë‘ ë§Œì¡±í•  ìˆ˜ ìˆë„ë¡ 'í‚¤ì¦ˆÂ·ì‹œë‹ˆì–´ ëª¨ë‘ OK' í”„ë¡œê·¸ë¨ì„ í•¨ê»˜ ë°°ì¹˜í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸ—ºï¸ ì—¬í–‰ ê°œìš”\n",
      "- **ê¸°ê°„**: 3ë°• 4ì¼(ì„œìš¸ ì¶œë°œÂ·ê·€ê°€ ê¸°ì¤€)\n",
      "- **ë™ì„ **: ì„œìš¸ â†’ (1ì‹œê°„ 30ë¶„) ì¶˜ì²œ(1ë°•) â†’ (1ì‹œê°„) ê°•ì´ŒÂ·ë‚¨ì´ì„¬(1ë°•) â†’ (1ì‹œê°„ 30ë¶„) ì–‘í‰(1ë°•) â†’ ì„œìš¸\n",
      "- **ì°¨ëŸ‰**: ìì°¨ or ë Œí„°ì¹´(2ì—´ ì—ì–´ì»¨Â·ì¹´ì‹œíŠ¸ í™•ë³´ í•„ìˆ˜)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      " [After] ëª¨ë¸ ê²°ê³¼\n",
      "ê°€ì¡±ê³¼ í•¨ê»˜ 3ë°• 4ì¼ ë™ì•ˆ í•œêµ­ì—ì„œ ì—¬ìœ ë¡­ê²Œ ì—¬í–‰í•  ìˆ˜ ìˆëŠ” ì¼ì •ì„ ì¶”ì²œí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. \n",
      "\n",
      "*   **ì—¬í–‰ì§€:** ì„œìš¸ \n",
      "*   **ì—¬í–‰ì¼ì •:** 3ë°• 4ì¼\n",
      "\n",
      "### 1ì¼ì°¨: ì„œìš¸\n",
      "\n",
      "*   **ì•„ì¹¨:** í˜¸í…” ì²´í¬ì¸ ë° íœ´ì‹ \n",
      "*   **ì˜¤ì „:** ì„œìš¸ì˜ ëŒ€í‘œì ì¸ ê´€ê´‘ì§€ì¸ **ê²½ë³µê¶** ë°©ë¬¸ \n",
      "*   **ì˜¤í›„:** **ê²½ë³µê¶** ê·¼ì²˜ì˜ **ì¸ì‚¬ë™ ê±°ë¦¬** ì‚°ì±… \n",
      "*   **ì €ë…:** **ì¸ì‚¬ë™ ê±°ë¦¬** ê·¼ì²˜ì˜ ë§›ìˆëŠ” í•œì‹ë‹¹ì—ì„œ ì €ë… ì‹ì‚¬ \n",
      "\n",
      "### 2ì¼ì°¨: ì„œìš¸\n",
      "\n",
      "*   **ì•„ì¹¨:** **ë¶ì´Œ í•œì˜¥ë§ˆì„** ë°©ë¬¸ \n",
      "*   **ì˜¤í›„:** **ì„œìš¸ ìˆ²** ì‚°ì±… \n",
      "*   **ì €ë…:** **ì„œìš¸ ìˆ²** ê·¼ì²˜ì˜ ë§›ìˆëŠ” ì‹ë‹¹ì—ì„œ ì €ë… ì‹ì‚¬ \n",
      "\n",
      "### 3ì¼ì°¨: ì„œìš¸\n",
      "\n",
      "*   **ì•„ì¹¨:** **ë¡¯ë°ì›”ë“œ** ë°©ë¬¸ \n",
      "*   **ì˜¤í›„:** **ë¡¯ë°ì›”ë“œ** ê·¼ì²˜ì˜ **ì„œìš¸ëœë“œ** ë°©ë¬¸ \n",
      "*   **ì €ë…:** **ì„œìš¸ëœë“œ** ê·¼ì²˜ì˜ ë§›ìˆëŠ” ì‹ë‹¹ì—ì„œ ì €ë… ì‹ì‚¬ \n",
      "\n",
      "### 4ì¼ì°¨: ì„œìš¸\n",
      "\n",
      "*   **ì•„ì¹¨:** **ê´‘í™”ë¬¸ ê±°ë¦¬** ë°©ë¬¸ \n",
      "*   **ì˜¤í›„:** **ê´‘í™”ë¬¸ ê±°ë¦¬** ê·¼ì²˜ì˜ **êµ­ë¦½ì¤‘ì•™ë°•ë¬¼ê´€** ë°©ë¬¸ \n",
      "*   **ì €ë…:** **êµ­ë¦½ì¤‘ì•™ë°•ë¬¼ê´€** ê·¼ì²˜ì˜ ë§›ìˆëŠ” ì‹ë‹¹ì—ì„œ ì €ë… ì‹ì‚¬ \n",
      "\n",
      "ì´ ì¼ì •ì€ ì„œìš¸ì˜ ë‹¤ì–‘í•œ ê´€ê´‘ì§€ë¥¼ ë°©ë¬¸í•˜ë©´ì„œë„ ì—¬ìœ ë¡œìš´ ì‹œê°„ì„ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ê´€ê´‘ì§€ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ëŒ€ì¤‘êµí†µì„ ì´ìš©í•˜ì—¬ ì´ë™í•  ìˆ˜ ìˆìœ¼ë©°, ë§›ìˆëŠ” ì‹ë‹¹ê³¼ ì¹´í˜ë¥¼ ë°©ë¬¸í•˜ì—¬ ì—¬ìœ ë¡œìš´ ì‹œê°„ì„ ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "#  ëª¨ë¸ íŒŒë¼ë¯¸í„° (Before: ê¸°ë³¸ì ì¸ ì¶”ì²œ / After: ë§ì¶¤í˜• ì„¸ë¶€ ì •ë³´ ì¶”ê°€)\n",
    "before_params = {\n",
    "    \"temperature\": 0.7,         # ëœë¤ì„±ì„ ì ì ˆíˆ ìœ ì§€ (ë‹¤ì–‘í•œ ë‹µë³€ ìœ ë„)\n",
    "    \"max_tokens\": 300,          # ì‘ë‹µ ê¸¸ì´ë¥¼ ì ì ˆíˆ ì¡°ì ˆ\n",
    "    \"frequency_penalty\": 0.5,   # ë°˜ë³µ ë‹¨ì–´ ì–µì œ\n",
    "    \"presence_penalty\": 0.5,    # ìƒˆë¡œìš´ ë‹¨ì–´ í¬í•¨ ì¥ë ¤\n",
    "}\n",
    "\n",
    "after_params = {\n",
    "    \"temperature\": 0.3,         # ì°½ì˜ì„±ì„ ë‚®ì¶”ê³  ì •í™•í•œ ë‹µë³€ ìœ ë„\n",
    "    \"max_tokens\": 800,          # ë” ê¸´ ë‹µë³€ì„ ìƒì„± (ì„¸ë¶€ ì •ë³´ í¬í•¨)\n",
    "    \"top_p\": 0.85,              # í™•ë¥  ê¸°ë°˜ ìƒ˜í”Œë§ (ì¼ê´€ì„±ê³¼ ë‹¤ì–‘ì„± ê· í˜•)\n",
    "    \"frequency_penalty\": 0.2,   # ë°˜ë³µ ë‹¨ì–´ ê°ì†Œ (ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€)\n",
    "    \"presence_penalty\": 0.3,    # ìƒˆë¡œìš´ ì •ë³´ í¬í•¨ ì¥ë ¤\n",
    "}\n",
    "\n",
    "#  ë‘ ê°œì˜ ëª¨ë¸ ìƒì„± (Before / After)\n",
    "#before_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", **before_params)\n",
    "before_model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    **before_params\n",
    ")\n",
    "#after_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", **after_params)\n",
    "after_model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    **after_params\n",
    ")\n",
    "\n",
    "#  í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì˜¬ë°”ë¥¸ ChatMessagePromptTemplate ì‚¬ìš©)\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"ë‹¹ì‹ ì€ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë§ëŠ” ìµœì ì˜ ì—¬í–‰ì§€ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "user_message = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, user_message])\n",
    "\n",
    "#  ì²´ì¸ ìƒì„± (Before / After)\n",
    "before_chain = chat_prompt | before_model\n",
    "after_chain = chat_prompt | after_model\n",
    "\n",
    "#  ì§ˆë¬¸ ì„¤ì • (Before / Afterì˜ ì°¨ì´ì ì„ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ ë³€ê²½)\n",
    "test_question = \"ê°€ì¡±ê³¼ í•¨ê»˜ 3ë°• 4ì¼ ë™ì•ˆ í•œêµ­ì—ì„œ ì—¬ìœ ë¡­ê²Œ ì—¬í–‰í•  ìˆ˜ ìˆëŠ” ì¼ì •ì„ ë™ì„ ì„ ê³ ë ¤í•˜ì—¬ ìì„¸í•˜ê²Œ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "#  Before ëª¨ë¸ ì‹¤í–‰\n",
    "before_response = before_chain.invoke({\"user_input\": test_question})\n",
    "\n",
    "#  After ëª¨ë¸ ì‹¤í–‰\n",
    "after_response = after_chain.invoke({\"user_input\": test_question})\n",
    "\n",
    "#  ê²°ê³¼ ì¶œë ¥\n",
    "print(\" [Before] ëª¨ë¸ ê²°ê³¼\")\n",
    "print(before_response.content)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")  # ê°€ë…ì„±ì„ ìœ„í•œ êµ¬ë¶„ì„ \n",
    "print(\" [After] ëª¨ë¸ ê²°ê³¼\")\n",
    "print(after_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2) ëª¨ë¸ì— ì¶”ê°€ì ì¸ íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬\n",
    "- bind() ë©”ì„œë“œë¥¼ ì‚¬ìš©\n",
    "- max_tokens=50 (ê¸°ë³¸) / max_tokens=150 (ìƒì„¸í•œ ì„¤ëª…) \n",
    "- stop=[\".\"] â†’ ì²« ë²ˆì§¸ ë§ˆì¹¨í‘œì—ì„œ ì‘ë‹µì„ ì¤‘ë‹¨\n",
    "- temperature=0.8 â†’ ë³´ë‹¤ ì°½ì˜ì ì¸ ë‹µë³€ì„ ìƒì„±í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#  í”„ë¡¬í”„íŠ¸ ì„¤ì • (ì²œë¬¸í•™ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œ)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ì´ ì‹œìŠ¤í…œì€ ì²œë¬¸í•™ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "#  ê¸°ë³¸ ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ì ì¸ ë‹µë³€ ìƒì„±)\n",
    "#base_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=150)  # 150 í† í° ì œí•œ\n",
    "base_model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "#  ì§ˆë¬¸ ì„¤ì •\n",
    "# 1. MAX_TOKENS ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ëŠ” ì§ˆë¬¸ (ê¸¸ì´ ì œí•œ íš¨ê³¼)\n",
    "max_tokens_question = \"ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ì´ ë¯¸ë˜ ì‚¬íšŒì— ë¯¸ì¹  ì˜í–¥ì„ ê¸ì •ì  ì¸¡ë©´ê³¼ ë¶€ì •ì  ì¸¡ë©´ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# 2. STOP íŒŒë¼ë¯¸í„° ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ëŠ” ì§ˆë¬¸ (ì¤‘ë‹¨ì  íš¨ê³¼)\n",
    "stop_question = \"Python í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ëŠ” ì´ˆë³´ìì—ê²Œ ì¶”ì²œí•˜ëŠ” í•™ìŠµ ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”. ê° ë‹¨ê³„ë³„ë¡œ êµ¬ì²´ì ì¸ ë°©ë²•ê³¼ íŒì„ í¬í•¨í•´ì„œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# 3. TEMPERATURE ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ëŠ” ì§ˆë¬¸ (ì°½ì˜ì„± vs ì •í™•ì„±)\n",
    "temperature_question = \"ì‹œê°„ ì—¬í–‰ì´ ê°€ëŠ¥í•˜ë‹¤ë©´ ê³¼ê±°ì˜ ì–´ëŠ ì‹œëŒ€ë¡œ ê°€ê³  ì‹¶ì€ì§€ì™€ ê·¸ ì´ìœ ë¥¼ ì°½ì˜ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# 4. ë³µí•©ì  ë¹„êµë¥¼ ìœ„í•œ ì§ˆë¬¸ (ëª¨ë“  íŒŒë¼ë¯¸í„° íš¨ê³¼)\n",
    "complex_question = \"í™”ì„±ì— ì¸ë¥˜ê°€ ì •ì°©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê¸°ìˆ ê³¼ ì¤€ë¹„ì‚¬í•­ë“¤ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ê³ , ê° ë‹¨ê³„ì—ì„œ ì˜ˆìƒë˜ëŠ” ë„ì „ê³¼ì œì™€ í•´ê²°ë°©ì•ˆì„ ì œì‹œí•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "question = stop_question\n",
    "\n",
    "#  Before (ê¸°ë³¸ max_tokens=150)\n",
    "messages = prompt.format_messages(user_input=question)\n",
    "before_answer = base_model.invoke(messages)\n",
    "\n",
    "#  Before ì¶œë ¥\n",
    "print(\"\\n [Before] ê¸°ë³¸ max_tokens=150 (ê¸°ë³¸ ë‹µë³€)\")\n",
    "print(before_answer.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")  # ê°€ë…ì„±ì„ ìœ„í•œ êµ¬ë¶„ì„ \n",
    "\n",
    "#  After (íŒŒë¼ë¯¸í„° ì¡°ì • í›„ ë¹„êµ)\n",
    "stop_chain = prompt | base_model.bind(max_tokens=150, stop=[\".\"])  # ì²« ë²ˆì§¸ ë§ˆì¹¨í‘œì—ì„œ ì¤‘ë‹¨\n",
    "temp_chain = prompt | base_model.bind(max_tokens=150, temperature=0.8)  # ì°½ì˜ì ì¸ ë‹µë³€ ìœ ë„\n",
    "\n",
    "stop_answer = stop_chain.invoke({\"user_input\": question})\n",
    "temp_answer = temp_chain.invoke({\"user_input\": question})\n",
    "\n",
    "#  After ì¶œë ¥ (stop vs temperature ë¹„êµ)\n",
    "print(\" [After] max_tokens=150, stop=['.'] (ì²« ë²ˆì§¸ ë§ˆì¹¨í‘œì—ì„œ ì‘ë‹µ ì¤‘ë‹¨)\")\n",
    "print(stop_answer.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")  # ê°€ë…ì„±ì„ ìœ„í•œ êµ¬ë¶„ì„ \n",
    "\n",
    "print(\" [After] max_tokens=150, temperature=0.8 (ì°½ì˜ì ì¸ ë‹µë³€)\")\n",
    "print(temp_answer.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
