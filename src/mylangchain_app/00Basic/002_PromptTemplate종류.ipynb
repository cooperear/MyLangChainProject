{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate \n",
    "* [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate)\n",
    "* [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate)\n",
    "* [ChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html#langchain_core.prompts.chat.ChatMessagePromptTemplate)\n",
    "* [FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html#langchain_core.prompts.few_shot.FewShotPromptTemplate)\n",
    "* PartialPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) PromptTemplate ì˜ from_template() í•¨ìˆ˜ ì‚¬ìš©\n",
    "* ì£¼ë¡œ LLM(í…ìŠ¤íŠ¸ ì™„ì„±í˜• ëª¨ë¸, ex. Ollama, GPT-3.5)ê³¼ í•¨ê»˜ ì‚¬ìš©\n",
    "* í•˜ë‚˜ì˜ ë¬¸ìì—´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ChatGPTëŠ” ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ **ìê¸°ì§€ë„í•™ìŠµ**(selfâ€‘supervised learning) ë°©ì‹ìœ¼ë¡œ '\n",
      " 'ì‚¬ì „í•™ìŠµ(preâ€‘training)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ëª¨ë¸ì€ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•˜ë©´ì„œ ì–¸ì–´ì˜ í†µê³„ì  íŒ¨í„´ê³¼ ì˜ë¯¸ '\n",
      " 'ê´€ê³„ë¥¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ì— ì••ì¶•í•©ë‹ˆë‹¤. ì´í›„ì—ëŠ” ì¸ê°„ì´ ë§Œë“  ì§ˆë¬¸â€‘ë‹µë³€ ìŒ ë“±ìœ¼ë¡œ **ì§€ë„í•™ìŠµ**(fineâ€‘tuning)ê³¼ '\n",
      " '**ê°•í™”í•™ìŠµ**(RLHF)ì„ ì ìš©í•´, ë³´ë‹¤ ì •í™•í•˜ê³  ì•ˆì „í•œ ëŒ€í™” ëŠ¥ë ¥ì„ ê°–ì¶”ê²Œ ë©ë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pprint import pprint\n",
    "\n",
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    # model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    " model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) PromptTemplate ê²°í•©í•˜ê¸°\n",
    "* ë™ì¼í•œ Prompt íŒ¨í„´ì„ ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ëŸ¬ ê°œì˜ ì§ˆë¬¸ì„ ì‘ì„±í•´ì„œ LLMì„ ì‹¤í–‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['count', 'language', 'model_name'] input_types={} partial_variables={} template='{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\\n\\n ê·¸ë¦¬ê³  {model_name} ëª¨ë¸ì˜ ì¥ì ì„ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”\\n\\n {model_name} ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”? ëª¨ë¸ëª…ì€ {language}ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.'\n",
      "('**ChatGPT ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ (3ë¬¸ì¥)**  \\n'\n",
      " '1. ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ í† í° ê°„ì˜ í™•ë¥ ì  ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³ , ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ì‚¬ì „ í•™ìŠµ(preâ€‘training)ì„ '\n",
      " 'ì§„í–‰í•©ë‹ˆë‹¤.  \\n'\n",
      " '2. ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì¸ê°„ì´ ë§Œë“  ì§ˆë¬¸â€‘ë‹µë³€ ìŒì´ë‚˜ ëŒ€í™” ë¡œê·¸ ë“±ìœ¼ë¡œ ëª¨ë¸ì˜ ì‘ë‹µ í’ˆì§ˆì„ ë†’ì´ëŠ” ì§€ë„ '\n",
      " 'í•™ìŠµ(supervised fineâ€‘tuning)ê³¼, ëª¨ë¸ ìì²´ê°€ ìƒì„±í•œ ë‹µë³€ì„ í‰ê°€í•´ ë³´ìƒ ì‹ í˜¸ë¥¼ ì£¼ëŠ” ê°•í™” í•™ìŠµ(RLHF)ì„ '\n",
      " 'ìˆ˜í–‰í•©ë‹ˆë‹¤.  \\n'\n",
      " '3. ìµœì¢…ì ìœ¼ë¡œ ì—¬ëŸ¬ ë‹¨ê³„ì˜ ë¯¸ì„¸ ì¡°ì •ì„ ê±°ì³, ë‹¤ì–‘í•œ ì£¼ì œì™€ ë¬¸ë§¥ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ ìµœì í™”ë©ë‹ˆë‹¤.  \\n'\n",
      " '\\n'\n",
      " '**ChatGPT ëª¨ë¸ì˜ ì¥ì  ìš”ì•½**  \\n'\n",
      " '- **ë²”ìš©ì„±**: ê´‘ë²”ìœ„í•œ ë¶„ì•¼ì˜ ì§€ì‹ê³¼ ì–¸ì–´ íŒ¨í„´ì„ í•™ìŠµí•´ ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•œë‹¤.  \\n'\n",
      " '- **ëŒ€í™” íë¦„ ìœ ì§€**: ë¬¸ë§¥ì„ ê¸°ì–µí•˜ê³  ì´ì „ ë°œì–¸ì„ ê³ ë ¤í•´ ì¼ê´€ëœ ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆë‹¤.  \\n'\n",
      " '- **ë¹ ë¥¸ ì‘ë‹µ**: ëŒ€ê·œëª¨ ë³‘ë ¬ ì—°ì‚°ê³¼ ìµœì í™”ëœ ì¶”ë¡  êµ¬ì¡° ë•ë¶„ì— ì‹¤ì‹œê°„ì— ê°€ê¹Œìš´ ì†ë„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.  \\n'\n",
      " '\\n'\n",
      " '**ChatGPTì™€ ë¹„ìŠ·í•œ AI ëª¨ë¸ (ì˜ì–´ ëª¨ë¸ëª…)**  \\n'\n",
      " '- GPTâ€‘4 (OpenAI)  \\n'\n",
      " '- LLaMA 2 (Meta)  \\n'\n",
      " '- Claude (Anthropic)  \\n'\n",
      " '- Gemini (Google DeepMind)  \\n'\n",
      " '- Mistral (Mistral AI)  \\n'\n",
      " '- Gemini Flash (Google)  \\n'\n",
      " '- Falcon (Technology Innovation Institute)  \\n'\n",
      " '- BLOOM (BigScience)  \\n'\n",
      " '- PaLM 2 (Google)  \\n'\n",
      " '- Gemini Pro (Google)  ')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# í…œí”Œë¦¿ì— ê°’ì„ ì±„ì›Œì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# ë¬¸ìì—´ í…œí”Œë¦¿ ê²°í•© (PromptTemplate + PromptTemplate + ë¬¸ìì—´)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n ê·¸ë¦¬ê³  {model_name} ëª¨ë¸ì˜ ì¥ì ì„ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”\")\n",
    "              + \"\\n\\n {model_name} ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”? ëª¨ë¸ëª…ì€ {language}ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"ì˜ì–´\")\n",
    "print(combined_prompt)\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"ì˜ì–´\"})\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë°°ì—´ í˜•íƒœë¡œ í•˜ì—¬ ì—¬ëŸ¬ê°œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 2 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'Gemini ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.']\n",
      "<class 'str'> GPT-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 2 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "('GPT-4ëŠ” ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ëŠ” **ìê¸°ì§€ë„í•™ìŠµ**(selfâ€‘supervised '\n",
      " 'learning) ë°©ì‹ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ì´í›„ì—ëŠ” ì¸ê°„ì´ ë§Œë“  ì§ˆë¬¸â€‘ë‹µë³€ ìŒì´ë‚˜ í”¼ë“œë°±ì„ í™œìš©í•´ **ê°•í™”í•™ìŠµ**(RLHF)ìœ¼ë¡œ '\n",
      " 'ëª¨ë¸ì˜ ì¶œë ¥ í’ˆì§ˆê³¼ ì•ˆì „ì„±ì„ ì¡°ì •í•©ë‹ˆë‹¤.')\n",
      "<class 'str'> Gemini ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "('Gemini ëª¨ë¸ì€ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ì™€ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°(ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“±)ë¥¼ ì‚¬ìš©í•´ ì‚¬ì „ í•™ìŠµ(preâ€‘training)ëœ ê±°ëŒ€ ì–¸ì–´Â·ë¹„ì „ '\n",
      " 'ëª¨ë¸ì…ë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” ìê¸°ì§€ë„ í•™ìŠµ(selfâ€‘supervised learning)ê³¼ ëŒ€ê·œëª¨ ìƒ˜í”Œë§ì„ í†µí•´ ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ '\n",
      " 'í™•ë¥ ì  ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³ , ì´í›„ íŠ¹ì • ì‘ì—…ì— ë§ì¶° ì†ŒëŸ‰ì˜ ë¼ë²¨ ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •(fineâ€‘tuning)í•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œëŠ” ì¸ê°„ í”¼ë“œë°±ì„ '\n",
      " 'í™œìš©í•œ ê°•í™”í•™ìŠµ(RLHF) ë“±ì„ ì ìš©í•´ ì‘ë‹µ í’ˆì§ˆê³¼ ì•ˆì „ì„±ì„ ë†’ì…ë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemini\", \"count\": 3},\n",
    "]\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ ìƒì„±\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # ë¯¸ë¦¬ ìƒì„±ëœ ì§ˆë¬¸ ëª©ë¡ í™•ì¸\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    print(type(prompt), prompt)\n",
    "    response = llm.invoke(prompt)\n",
    "    pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) ChatPromptTemplate\n",
    "* Tuple í˜•íƒœì˜ system, user, assistant ë©”ì‹œì§€ ì§€ì›\n",
    "* ì—¬ëŸ¬ ê°œì˜ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬ ê°€ëŠ¥\n",
    "* ê°„ê²°ì„±ê³¼ ê°€ë…ì„±ì´ ë†’ê³  ë‹¨ìˆœí•œ êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='This system is an expert in answering questions about AI. Please provide clear and detailed explanations.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ChatGPT ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ChatGPTëŠ” ì˜¤í”ˆAIì‚¬ê°€ ê°œë°œí•œ ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì…ë‹ˆë‹¤. ChatGPTì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ChatGPTëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ë¡œì„œ, ìì—°ì–´ ì²˜ë¦¬(NLP, Natural Language Processing) ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ChatGPTëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì›ë¦¬ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘**: ChatGPTëŠ” ì¸í„°ë„·ì—ì„œ ìˆ˜ì§‘ëœ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ì±…, ê¸°ì‚¬, ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸, ì†Œì…œ ë¯¸ë””ì–´ ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘ë©ë‹ˆë‹¤.\n",
      "\n",
      "2. **í† í°í™”**: ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” í† í°ì´ë¼ëŠ” ë‹¨ìœ„ë¡œ ë¶„í• ë©ë‹ˆë‹¤. í† í°ì€ ë‹¨ì–´, êµ¬ì ˆ, ë˜ëŠ” ê¸°í˜¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ì„ë² ë”©**: ê° í† í°ì€ ë²¡í„° ê³µê°„ì— ì„ë² ë”©ë©ë‹ˆë‹¤. ì„ë² ë”©ì€ í† í°ì˜ ì˜ë¯¸ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ë²¡í„°ëŠ” í† í°ì˜ ì˜ë¯¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ì ê°’ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n",
      "\n",
      "4. **íŠ¸ëœìŠ¤í¬ë¨¸**: ChatGPTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ë¼ëŠ” ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì…ë ¥ í† í°ì˜ ì„ë² ë”©ì„ ì²˜ë¦¬í•˜ì—¬ ì¶œë ¥ í† í°ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§**: ChatGPTëŠ” ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§ì´ë¼ëŠ” í•™ìŠµ ëª©í‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ëª©í‘œëŠ” ì…ë ¥ í† í°ì˜ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ë¦¬ê³ , ê°€ë ¤ì§„ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "6. **ë‹¤ìŒ í† í° ì˜ˆì¸¡**: ChatGPTëŠ” ì´ì „ í† í°ì˜ ì„ë² ë”©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ ì˜ˆì¸¡ì€ í™•ë¥  ë¶„í¬ë¡œ í‘œí˜„ë˜ë©°, ëª¨ë¸ì€ ì´ í™•ë¥  ë¶„í¬ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "7. **ì†ì‹¤ í•¨ìˆ˜**: ChatGPTëŠ” ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤. ì†ì‹¤ í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œ í† í° ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
      "\n",
      "8. **ìµœì í™”**: ChatGPTëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
      "\n",
      "9. **ë°˜ë³µ í•™ìŠµ**: ChatGPTëŠ” ìœ„ì˜ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤. ë°˜ë³µ í•™ìŠµì„ í†µí•´ ëª¨ë¸ì€ ì ì  ë” ì •í™•í•œ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
      "\n",
      "ChatGPTì˜ í•™ìŠµ ì›ë¦¬ëŠ” ì´ë ‡ìŠµë‹ˆë‹¤. ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ë³µì¡í•œ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 2-íŠœí”Œ í˜•íƒœì˜ ë©”ì‹œì§€ ëª©ë¡ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„± (type, content)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    # role, message\n",
    "    (\"system\", \"This system is an expert in answering questions about {topic}. Please provide clear and detailed explanations.\"),\n",
    "    (\"human\", \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", model_name=\"ChatGPT\")\n",
    "print(messages)\n",
    "\n",
    "# ìƒì„±í•œ ë©”ì‹œì§€ë¥¼ ë°”ë¡œ ì£¼ì…í•˜ì—¬ í˜¸ì¶œí•˜ê¸°\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´ì¸ì„ ìƒì„±í•˜ì—¬ í˜¸ì¶œí•˜ê¸°\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"topic\":\"AI\", \"model_name\":\"ChatGPT\"})\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) ChatPromptTemplate\n",
    "* SystemMessagePromptTemplateì™€ HumanMessagePromptTemplate í´ë˜ìŠ¤ ì‚¬ìš©\n",
    "* ê°ì²´ ì§€í–¥ì  ì ‘ê·¼ - Message ê°ì²´ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìƒì„± ê°€ëŠ¥\n",
    "* ì—¬ëŸ¬ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„ íƒ\n",
    "\n",
    "```python\n",
    "if user_is_beginner:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª…: {topic}\")\n",
    "else:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ìƒì„¸ ë¶„ì„: {topic}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm happy to provide more information on the topic.\n",
      "\n",
      "**Deep Learning** is a subset of **Machine Learning** (ML) that involves the use of **Artificial Neural Networks** (ANNs) to analyze data. Inspired by the structure and function of the human brain, ANNs are composed of layers of interconnected nodes or **neurons** that process and transform inputs into meaningful representations.\n",
      "\n",
      "**Key Characteristics:**\n",
      "\n",
      "1. **Multiple Layers**: Deep learning models have multiple layers of neurons, which allow them to learn complex patterns and relationships in data. These layers are typically organized in a hierarchical manner, with early layers learning low-level features and later layers learning high-level features.\n",
      "2. **Hierarchical Representations**: Each layer in a deep learning model learns to represent the input data in a more abstract and meaningful way. This hierarchical representation enables the model to capture complex patterns and relationships in the data.\n",
      "3. **Distributed Representations**: Deep learning models use distributed representations, where each piece of information is represented across multiple neurons. This allows the model to capture multiple aspects of the data and to generalize well to new, unseen data.\n",
      "\n",
      "**Types of Deep Learning Models:**\n",
      "\n",
      "1. **Convolutional Neural Networks (CNNs)**: Designed for image and video processing tasks, CNNs use convolutional and pooling layers to extract features from data.\n",
      "2. **Recurrent Neural Networks (RNNs)**: Suitable for sequential data, RNNs use recurrent connections to model temporal relationships in data.\n",
      "3. **Autoencoders**: Used for unsupervised learning tasks, autoencoders learn to compress and reconstruct data.\n",
      "\n",
      "**Applications of Deep Learning:**\n",
      "\n",
      "1. **Computer Vision**: Image classification, object detection, segmentation, and generation.\n",
      "2. **Natural Language Processing (NLP)**: Text classification, language modeling, machine translation, and text generation.\n",
      "3. **Speech Recognition**: Speech-to-text systems.\n",
      "4. **Robotics**: Control and navigation systems.\n",
      "\n",
      "**Real-World Examples:**\n",
      "\n",
      "1. **Virtual Assistants**: Siri, Google Assistant, and Alexa use deep learning for speech recognition and natural language processing.\n",
      "2. **Image Recognition**: Facebook and Google Photos use deep learning for image classification and object detection.\n",
      "3. **Self-Driving Cars**: Companies like Waymo and Tesla use deep learning for computer vision and control systems.\n",
      "\n",
      "**Challenges and Limitations:**\n",
      "\n",
      "1. **Large Datasets**: Deep learning models require large amounts of labeled data to train effectively.\n",
      "2. **Computational Resources**: Training deep learning models can be computationally expensive and require significant resources.\n",
      "3. **Interpretability**: Deep learning models can be difficult to interpret and understand, making it challenging to explain their decisions.\n",
      "\n",
      "Overall, deep learning has revolutionized many fields and has enabled significant advances in areas like computer vision, NLP, and robotics. Its applications continue to grow, and it is an exciting and rapidly evolving field of research.\n"
     ]
    }
   ],
   "source": [
    "# ChatMessagePromptTemplate í™œìš©\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê°œë³„ ë©”ì‹œì§€ í…œí”Œë¦¿ ì •ì˜\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI expert in {topic}. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplateë¡œ ë©”ì‹œì§€ë“¤ì„ ë¬¶ê¸°\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒì„±\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"What is deep learning?\")\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatMessagePromptTemplateëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë©”ì‹œì§€(ì‹œìŠ¤í…œ, ì¸ê°„, AI)ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "* SystemMessagePromptTemplate: ì´ í…œí”Œë¦¿ì€ AI ëª¨ë¸ì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ê±°ë‚˜ ì „ë°˜ì ì¸ ê·œì¹™ì„ ì„¤ì •í•˜ëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤. ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” \"ë²ˆì—­ì„ ë„ì™€ì£¼ëŠ” ìœ ìš©í•œ ë„ìš°ë¯¸\"ë¼ëŠ” ì—­í• ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "* HumanMessagePromptTemplate: ì´ í…œí”Œë¦¿ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ë‹´ëŠ” ì¸ê°„ ë©”ì‹œì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì•„ë˜ì˜ ì˜ˆì‹œì—ì„œëŠ” ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.\n",
    "* ChatPromptTemplate.from_messages: ì´ í´ë˜ìŠ¤ ë©”ì„œë“œëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì¸ê°„ ë©”ì‹œì§€ ë“± ì—¬ëŸ¬ ì¢…ë¥˜ì˜ MessagePromptTemplate ê°ì²´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ í•˜ë‚˜ì˜ ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.\n",
    "* format_messages: ì´ ë©”ì„œë“œëŠ” ì •ì˜ëœ í…œí”Œë¦¿ì— ì‹¤ì œ ê°’ì„ ì±„ì›Œ ë„£ì–´ [SystemMessage, HumanMessage] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ ë¦¬ìŠ¤íŠ¸ëŠ” ì±„íŒ… ëª¨ë¸(Chat Model) ì— ë°”ë¡œ ì „ë‹¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant that translates English to Korean.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n",
      "ë‚˜ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ì¢‹ì•„í•´ìš”.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# 1. SystemMessagePromptTemplateì™€ HumanMessagePromptTemplate ìƒì„±\n",
    "# SystemMessagePromptTemplateëŠ” ëª¨ë¸ì˜ í˜ë¥´ì†Œë‚˜ ë˜ëŠ” ê¸°ë³¸ ì§€ì¹¨ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# HumanMessagePromptTemplateëŠ” ì‚¬ìš©ìë¡œë¶€í„° ë°›ëŠ” ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "human_template = \"{text_to_translate}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# 2. ChatPromptTemplate ìƒì„±\n",
    "# ìœ„ì—ì„œ ë§Œë“  ë‘ í…œí”Œë¦¿ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ ChatPromptTemplateì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 3. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "# chat_prompt_template.format_messages()ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# ì´ í•¨ìˆ˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì…ë ¥ ë³€ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.\n",
    "formatted_prompt = chat_prompt_template.format_messages(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"Korean\",\n",
    "    text_to_translate=\"I love programming.\"\n",
    ")\n",
    "\n",
    "# 4. ê²°ê³¼ ì¶œë ¥\n",
    "print(formatted_prompt)\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) FewShotPromptTemplate\n",
    "* FewShotPromptTemplateì€ ëª¨ë¸ì´ íŠ¹ì • í˜•ì‹ì„ ë”°ë¥´ê²Œ í•˜ê±°ë‚˜, ì¼ê´€ëœ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "* ë„ë©”ì¸ ì§€ì‹ì´ í•„ìš”í•˜ê±°ë‚˜, AIê°€ ì˜¤ë‹µì„ ì¤„ì´ê³  ë” ì‹ ë¢°í•  ë§Œí•œ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•´ì•¼ í•  ë•Œ íš¨ê³¼ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-1) PromptTemplateì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "## íƒœì–‘ê³„ í–‰ì„± ê°„ëµ ì •ë¦¬\n",
      "\n",
      "| ìˆœì„œ | í–‰ì„± | íŠ¹ì§•Â·ì£¼ìš” ì •ë³´ |\n",
      "|------|------|----------------|\n",
      "| 1 | **ìˆ˜ì„± (Mercury)** | - íƒœì–‘ì— ê°€ì¥ ê°€ê¹Œìš´ í–‰ì„±<br>- í‰ê·  ì§ê²½â€¯â‰ˆâ€¯4,880â€¯km (ì§€êµ¬ì˜ 38%)<br>- ëŒ€ê¸°ê°€ ê±°ì˜ ì—†ìœ¼ë©°, ë‚®ê³¼ ë°¤ ì˜¨ë„ ì°¨ê°€ ê·¹ì‹¬ (âˆ’173â€¯Â°C ~ +427â€¯Â°C) |\n",
      "| 2 | **ê¸ˆì„± (Venus)** | - â€œì§€êµ¬ì˜ ìŒë‘¥ì´â€ë¼ ë¶ˆë¦¬ì§€ë§Œ, ë‘êº¼ìš´ ì´ì‚°í™”íƒ„ì†Œ ëŒ€ê¸°ì™€ í™©ì‚° êµ¬ë¦„ìœ¼ë¡œ ì˜¨ì‹¤íš¨ê³¼ê°€ ê°•í•¨<br>- í‘œë©´ ì˜¨ë„â€¯â‰ˆâ€¯467â€¯Â°C (ì§€êµ¬ë³´ë‹¤ í›¨ì”¬ ëœ¨ê±°ì›€)<br>- ìì „ ë°©í–¥ì´ ì—­í–‰(ì‹œê³„ ë°˜ëŒ€) |\n",
      "| 3 | **ì§€êµ¬ (Earth)** | - ë¬¼ì´ ì•¡ì²´ ìƒíƒœë¡œ ì¡´ì¬í•˜ëŠ” ìœ ì¼í•œ ì•Œë ¤ì§„ í–‰ì„±<br>- í‰ê·  ì§ê²½â€¯â‰ˆâ€¯12,742â€¯km<br>- ëŒ€ê¸° êµ¬ì„±: Nâ‚‚â€¯â‰ˆâ€¯78%, Oâ‚‚â€¯â‰ˆâ€¯21% |\n",
      "| 4 | **í™”ì„± (Mars)** | - â€œë¶‰ì€ í–‰ì„±â€ì´ë¼ ë¶ˆë¦¬ëŠ” ì´ìœ ëŠ” ì² ì‚°í™”ë¬¼(ë…¹) ë•Œë¬¸ì—<br>- í‰ê·  ì§ê²½â€¯â‰ˆâ€¯6,779â€¯km (ì§€êµ¬ì˜ ì ˆë°˜)<br>- ì–‡ì€ COâ‚‚ ëŒ€ê¸°, ê·¹ì§€ì— ë¬¼ ì–¼ìŒ ì¡´ì¬ |\n",
      "| 5 | **ëª©ì„± (Jupiter)** | - íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„± (ì§ê²½â€¯â‰ˆâ€¯139,820â€¯km)<br>- ì£¼ì„±ë¶„ì€ ìˆ˜ì†ŒÂ·í—¬ë¥¨, ê°•ë ¥í•œ ìê¸°ì¥ ë³´ìœ <br>- 79ê°œì˜ ìœ„ì„±(2025ë…„ í˜„ì¬) ì¤‘ ê°ˆë¦´ë ˆì˜¤ ìœ„ì„±(ì´ì˜¤, ìœ ë¡œíŒŒ, ê°€ë‹ˆë©”ë°, ì¹¼ë¦¬ìŠ¤í† ) ìœ ëª… |\n",
      "| 6 | **í† ì„± (Saturn)** | - ì•„ë¦„ë‹¤ìš´ ê³ ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ ìœ ëª… (ì£¼ë¡œ ì–¼ìŒÂ·ì•”ì„ íŒŒí¸)<br>- ì§ê²½â€¯â‰ˆâ€¯116,460â€¯km, ë°€ë„ê°€ ë¬¼ë³´ë‹¤ ë‚®ì•„ ë¬¼ì— ëœ° ìˆ˜ ìˆìŒ<br>- ìœ„ì„± 80ì—¬ ê°œ, íƒ€ì´íƒ„ì€ ë‘êº¼ìš´ ëŒ€ê¸°ì™€ ë©”íƒ„ í˜¸ìˆ˜ë¥¼ ê°€ì§ |\n",
      "| 7 | **ì²œì™•ì„± (Uranus)** | - ì²­ë¡ìƒ‰ì„ ë¤ ë©”íƒ„ ê°€ìŠ¤ í’ë¶€<br>- ìì „ì¶•ì´ ì•½ 98Â°ë¡œ ê±°ì˜ ì˜†ìœ¼ë¡œ ëˆ„ì›Œ ìˆì–´ ê³„ì ˆ ë³€í™”ê°€ ê·¹ë‹¨ì <br>- ì§ê²½â€¯â‰ˆâ€¯50,724â€¯km, ìœ„ì„± 27ê°œ(í‹°íƒ„ia ë“±) |\n",
      "| 8 | **í•´ì™•ì„± (Neptune)** | - ê°€ì¥ ë°”ê¹¥ìª½ ì£¼ìš” í–‰ì„± (í”Œë£¨í† ëŠ” ì™œì†Œí–‰ì„±)<br>- ê°•í•œ ë°”ëŒ(ìµœëŒ€ 2,100â€¯km/h)ê³¼ ëŒ€ê¸° ì¤‘ ë©”íƒ„ìœ¼ë¡œ íŒŒë€ìƒ‰ ë‚˜íƒ€ë‚¨<br>- ì§ê²½â€¯â‰ˆâ€¯49,244â€¯km, ìœ„ì„± 14ê°œ(íŠ¸ë¦¬í†¤ ë“±) |\n",
      "\n",
      "### ì¶”ê°€ ì°¸ê³ \n",
      "- **ì™œì†Œí–‰ì„±**: í”Œë£¨í† , ì„¸ë ˆìŠ¤, í•˜ìš°ë©”ì•„, ë§ˆì¼€ë§ˆí¬ ë“±ì€ í–‰ì„± ê¸°ì¤€ì— ë¶€í•©í•˜ì§€ ì•Šì•„ â€˜ì™œì†Œí–‰ì„±â€™ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.  \n",
      "- **í–‰ì„± êµ¬ë¶„**: ë‚´í–‰ì„±(ìˆ˜ì„±Â·ê¸ˆì„±Â·ì§€êµ¬Â·í™”ì„±) â€“ ì•”ì„ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì‘ì€ í–‰ì„±; ì™¸í–‰ì„±(ëª©ì„±Â·í† ì„±Â·ì²œì™•ì„±Â·í•´ì™•ì„±) â€“ ê°€ìŠ¤Â·ì–¼ìŒ ê±°ì¸.  \n",
      "\n",
      "ì´ ì •ë„ë©´ íƒœì–‘ê³„ í–‰ì„±ë“¤ì˜ ê¸°ë³¸ì ì¸ íŠ¹ì§•ì„ ê°„ëµíˆ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplateì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# chain ì‹¤í–‰\n",
    "result = llm.invoke(\"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "print(type(result))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-2) FewShotChatMessagePromptTemplate ì‚¬ìš©í•˜ëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ìœ ì¹˜ì›ìƒë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ ì»¤ë®¤ë‹ˆì¼€ì´í„°ì…ë‹ˆë‹¤. ì•„ì´ì—ê²Œ ë§í•˜ëŠ” ë“¯í•œ ë¶€ë“œëŸ¬ìš´ ë§íˆ¬ë¥¼ ê°€ì§€ì£ '), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.', 'output': '### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\\n1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\\n2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\\n3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.'}, {'input': 'ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.', 'output': '### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\\n- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\\n- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\\n- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\\n- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002B14B0C5810>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002B14B0C6490>, root_client=<openai.OpenAI object at 0x000002B14B0C47D0>, root_async_client=<openai.AsyncOpenAI object at 0x000002B14B0C6210>, model_name='meta-llama/llama-4-scout-17b-16e-instruct', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
      "### íƒœì–‘ê³„ì˜ í–‰ì„±\n",
      "1. **ìˆ˜ì„±**: ê°€ì¥ ê°€ê¹Œìš´ í–‰ì„±ìœ¼ë¡œ, ë§¤ìš° ì‘ê³  ì˜¨ë„ê°€ ê·¹ì‹¬í•˜ê²Œ ë³€í•©ë‹ˆë‹¤.\n",
      "2. **ê¸ˆì„±**: ë°ê³  ëœ¨ê±°ìš´ í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "3. **ì§€êµ¬**: ìš°ë¦¬ê°€ ì‚¬ëŠ” ê³³ì…ë‹ˆë‹¤.\n",
      "4. **í™”ì„±**: ë¶‰ì€ í–‰ì„±ìœ¼ë¡œ, ë¡œë´‡ íƒì‚¬ê°€ í™œë°œí•©ë‹ˆë‹¤.\n",
      "5. **ëª©ì„±**: íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "6. **í† ì„±**: ì•„ë¦„ë‹¤ìš´ ê³ ë¦¬ë¥¼ ê°€ì§„ í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "7. **ì²œì™•ì„±**: ìì „ì¶•ì´ ì˜†ìœ¼ë¡œ ëˆ„ì›Œ ìˆëŠ” í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "8. **í•´ì™•ì„±**: ê°€ì¥ ë¨¼ í–‰ì„±ìœ¼ë¡œ, ë§¤ìš° ì¶¥ìŠµë‹ˆë‹¤.\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002B14B0C5810> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002B14B0C6490> root_client=<openai.OpenAI object at 0x000002B14B0C47D0> root_async_client=<openai.AsyncOpenAI object at 0x000002B14B0C6210> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# FewShotChatMessagePromptTemplate ì‚¬ìš©í•˜ëŠ” ê²½ìš°\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\n",
    "1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\n",
    "- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\n",
    "- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate ì ìš©\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ìœ ì¹˜ì›ìƒë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ ì»¤ë®¤ë‹ˆì¼€ì´í„°ì…ë‹ˆë‹¤. ì•„ì´ì—ê²Œ ë§í•˜ëŠ” ë“¯í•œ ë¶€ë“œëŸ¬ìš´ ë§íˆ¬ë¥¼ ê°€ì§€ì£ \"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° ì²´ì¸ êµ¬ì„±\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "chain = final_prompt | llm\n",
    "print(chain)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result = chain.invoke({\"input\": \"íƒœì–‘ê³„ì˜ í–‰ì„±ì€ ë­ê°€ ìˆì–´?\"})\n",
    "print(result.content)\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-1) PartialPrompt \n",
    "* í”„ë¡¬í”„íŠ¸ë¥¼ ë” ë™ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, AI ì‘ë‹µì„ ë” ì¼ê´€ì„± ìˆê²Œ ì¡°ì • ê°€ëŠ¥í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " í”„ë¡¬í”„íŠ¸: ê°€ì„ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ íƒœí’ ë°œìƒì´ ë§ë‚˜ìš”? ê°€ì„ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ì§€êµ¬ê³¼í•™ í˜„ìƒì„ 3ê°œ ì•Œë ¤ì£¼ì„¸ìš”\n",
      " ëª¨ë¸ ì‘ë‹µ: ê°€ì„ì— ë‚˜íƒ€ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ **íƒœí’**ë„ ë§ì§€ë§Œ, ê°€ì„ì€ ëŒ€ê¸°Â·í•´ì–‘Â·ì§€í‘œë©´ì´ ì„œë¡œ í¬ê²Œ ë³€ë™í•˜ëŠ” ì‹œê¸°ì´ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ ê°€ì§€ ë‹¤ë¥¸ í˜„ìƒë„ ë™ì‹œì— í™œë°œíˆ ì¼ì–´ë‚©ë‹ˆë‹¤. ì•„ë˜ì—ì„œëŠ” **ê°€ì„ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€**ë¥¼ ê°„ëµíˆ ì •ë¦¬í•˜ê³ , ê°ê°ì´ ì™œ ê°€ì„ì— ë‘ë“œëŸ¬ì§€ëŠ”ì§€ ì„¤ëª…ë“œë¦´ê²Œìš”.\n",
      "\n",
      "---\n",
      "\n",
      "## 1ï¸âƒ£ íƒœí’ (ì—´ëŒ€ ì €ê¸°ì••) â€“ â€œê°€ì„í˜• íƒœí’â€\n",
      "| íŠ¹ì§• | ê°€ì„ì— ë‘ë“œëŸ¬ì§€ëŠ” ì´ìœ  |\n",
      "|------|----------------------|\n",
      "| **ë°œë‹¬ ì¡°ê±´**: ë”°ëœ»í•œ í•´ìˆ˜ë©´(â‰¥ 26â€¯Â°C), ë†’ì€ ìˆ˜ë¶„ í•¨ëŸ‰, ì €ê¸°ì•• ì¤‘ì‹¬ì˜ ë°œë‹¬ | ê°€ì„ì´ ë˜ë©´ **ë™ì•„ì‹œì•„Â·ì„œíƒœí‰ì–‘**ì˜ í•´ìˆ˜ë©´ ì˜¨ë„ê°€ ì—¬ë¦„ ìµœê³ ì¹˜ë¥¼ ìœ ì§€í•˜ë©´ì„œë„, **ë™ìª½ìœ¼ë¡œ ì´ë™í•˜ëŠ” ì œíŠ¸ê¸°ë¥˜**ê°€ ê°•í•´ì ¸ì„œ íƒœí’ì´ ë¶ìª½(í•œë°˜ë„Â·ì¼ë³¸)ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì´ë™Â·ì „ê°œë©ë‹ˆë‹¤. |\n",
      "| **ê²½ë¡œ**: ì„œìª½ìœ¼ë¡œ ì´ë™ â†’ ë¶ë™ìª½ìœ¼ë¡œ íšŒì „ | ê³„ì ˆí’ì´ ë‚¨ìª½ì—ì„œ ë¶ìª½ìœ¼ë¡œ ë°”ë€ŒëŠ” **ë™í’ëŒ€(ë™ì•„ì‹œì•„Â·ë¶íƒœí‰ì–‘ ì œíŠ¸ê¸°ë¥˜)**ê°€ í˜•ì„±ë¼, íƒœí’ì´ ë‚¨ìª½ì—ì„œ ë¶ìª½ìœ¼ë¡œ â€˜ì „í™˜â€™í•˜ë©´ì„œ í•œë°˜ë„Â·ë™ë¶ì•„ ì§€ì—­ì— ì ‘ê·¼í•©ë‹ˆë‹¤. |\n",
      "| **í”¼í¬ ì‹œê¸°**: 9ì›”â€¯~â€¯10ì›” ì´ˆ | ì´ ì‹œê¸°ì— í•´ìˆ˜ë©´ ì˜¨ë„ê°€ ì•„ì§ ë†’ê³ , ëŒ€ê¸° ë¶ˆì•ˆì •ì„±ë„ ê°•í•´ íƒœí’ ë°œìƒ ë¹ˆë„ê°€ ìµœê³ ì¡°ì— ë‹¬í•©ë‹ˆë‹¤. |\n",
      "\n",
      "> **í•µì‹¬ ì •ë¦¬**: ê°€ì„ì€ â€œíƒœí’ì´ ê°€ì¥ ê°•í•˜ê³ , ë¶ìª½ìœ¼ë¡œ ì´ë™í•˜ê¸° ì‰¬ìš´ ì‹œê¸°â€ì´ë¯€ë¡œ, ê°€ì„í˜• íƒœí’ì´ ëŒ€í‘œì ì¸ ê°€ì„ í˜„ìƒì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 2ï¸âƒ£ ì¤‘ìœ„ë„ ì €ê¸°ì••Â·ì „ì„ ì„± í­í’ (ì‹œë² ë¦¬ì•„ ê³ ê¸°ì••Â·ë™ì•„ì‹œì•„ ì €ê¸°ì•• ì „ì´)\n",
      "| íŠ¹ì§• | ê°€ì„ì— ë‘ë“œëŸ¬ì§€ëŠ” ì´ìœ  |\n",
      "|------|----------------------|\n",
      "| **ì „í˜•ì ì¸ í˜•íƒœ**: ì°¨ê°€ìš´ ì‹œë² ë¦¬ì•„ ê³ ê¸°ì••ì´ ë‚¨í•˜í•˜ë©´ì„œ **ë™ì•„ì‹œì•„Â·ë¶íƒœí‰ì–‘ ì €ê¸°ì••**(ì¤‘ìœ„ë„ ì €ê¸°ì••)ê³¼ ë§Œë‚˜ ì „ì„ ì´ í˜•ì„±ë¨ | ê°€ì„ì—ëŠ” **ì‹œë² ë¦¬ì•„ ê³ ê¸°ì••ì´ ì ì°¨ ê°•í•´ì ¸ ë‚¨í•˜**í•˜ê³ , ë‚¨ìª½ì˜ ë”°ëœ»í•œ í•´ì–‘ì„± ê³µê¸°ì™€ ë§ˆì£¼í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë•Œ **ì „ì„ **(í•œë­Â·ì˜¨ë‚œ ì „ì„ )ì´ í˜•ì„±ë˜ê³ , ê°•ìˆ˜Â·ëŒí’Â·ë‡Œìš°ê°€ ë™ë°˜ëœ ì „ì„ ì„± í­í’ì´ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤. |\n",
      "| **ì£¼ìš” í˜„ìƒ**: ê¸‰ê²©í•œ ê¸°ì˜¨ ê°•í•˜, ê°•ìˆ˜Â·ëˆˆ, ëŒí’, ì•ˆê°œ | ì°¨ê°€ìš´ ëŒ€ë¥™ì„± ê³µê¸°ì™€ ë”°ëœ»í•œ í•´ì–‘ì„± ê³µê¸°ê°€ ê¸‰ê²©íˆ êµì°¨í•˜ë©´ì„œ **ëŒ€ê¸° ë¶ˆì•ˆì •**ì´ í¬ê²Œ ì¦ê°€í•˜ê³ , ì´ì— ë”°ë¼ **í­ìš°Â·í­ì„¤Â·ê°•í’**ì´ ë™ë°˜ë©ë‹ˆë‹¤. |\n",
      "| **í”¼í¬ ì‹œê¸°**: 9ì›”â€¯~â€¯11ì›” | ì „ì„  í™œë™ì´ ê°€ì¥ í™œë°œí•´ì§€ëŠ” ì‹œê¸°ë¡œ, íŠ¹íˆ **9~10ì›”**ì— ëŒ€ê·œëª¨ ì €ê¸°ì•• ì‹œìŠ¤í…œì´ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤. |\n",
      "\n",
      "> **í•µì‹¬ ì •ë¦¬**: ê°€ì„ì€ **ì‹œë² ë¦¬ì•„ ê³ ê¸°ì••ê³¼ í•´ì–‘ì„± ì €ê¸°ì••ì´ êµì°¨**í•˜ë©´ì„œ ì¤‘ìœ„ë„ ì €ê¸°ì••Â·ì „ì„ ì„± í­í’ì´ ë¹ˆë²ˆíˆ ì¼ì–´ë‚˜ëŠ” ì‹œê¸°ì…ë‹ˆë‹¤. ì´ëŠ” ê°•ìˆ˜Â·í­ì„¤Â·ëŒí’ ë“± ë‹¤ì–‘í•œ ê¸°ìƒ ì¬í•´ë¥¼ ë™ë°˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 3ï¸âƒ£ ì•ˆê°œÂ·ì˜¨ë„ ì—­ì „ í˜„ìƒ (ëŒ€ê¸° ì •ì²´Â·ë³µì‚¬ ëƒ‰ê°)\n",
      "| íŠ¹ì§• | ê°€ì„ì— ë‘ë“œëŸ¬ì§€ëŠ” ì´ìœ  |\n",
      "|------|----------------------|\n",
      "| **ì•ˆê°œ í˜•ì„± ë©”ì»¤ë‹ˆì¦˜**: ì§€í‘œë©´ì´ ê¸‰ê²©íˆ ëƒ‰ê°(ë³µì‚¬ ëƒ‰ê°) â†’ ê¸°ì˜¨ ì—­ì „ â†’ ìˆ˜ì¦ê¸°ê°€ ì‘ê²° | ê°€ì„ë°¤ì€ **ì¼ì¡°ëŸ‰ì´ ê°ì†Œ**í•˜ê³ , êµ¬ë¦„ì´ ì ì–´ ì§€í‘œë©´ì´ ë¹ ë¥´ê²Œ ë³µì‚¬ ëƒ‰ê°ë©ë‹ˆë‹¤. íŠ¹íˆ **ì‚°ê°„Â·ê°•ì›ë„Â·í•œê°• ìœ ì—­** ë“± ì €ì§€ëŒ€ì—ì„œ ì°¨ê°€ìš´ ê³µê¸°ê°€ í•˜ê°•í•˜ë©´ì„œ ì˜¨ë„ ì—­ì „ì´ ì¼ì–´ë‚˜ê³ , ì´ë•Œ **ì—°ë¬´Â·ë°©ì‚¬í˜• ì•ˆê°œ**ê°€ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤. |\n",
      "| **ëŒ€ê¸° ì •ì²´**: ê³ ê¸°ì••ì´ ìë¦¬ ì¡ê³  ë°”ëŒì´ ì•½í•´ì§ â†’ ìˆ˜ë¶„ì´ ì¶•ì ë¼ ì•ˆê°œ ë°œìƒ | ê°€ì„ì—ëŠ” **ê³ ê¸°ì••ì´ ì¥ê¸°ê°„ ë¨¸ë¬´ëŠ” ê²½ìš°ê°€ ë§ì•„** ë°”ëŒì´ ì•½í•´ì§€ê³  ëŒ€ê¸°ê°€ ì •ì²´ë©ë‹ˆë‹¤. ì´ë•Œ ìˆ˜ì¦ê¸°ê°€ ì¶•ì ë¼ ì•ˆê°œê°€ ì‰½ê²Œ í˜•ì„±ë©ë‹ˆë‹¤. |\n",
      "| **í”¼í¬ ì‹œê¸°**: 9ì›”â€¯~â€¯11ì›”, íŠ¹íˆ **ë°¤Â·ìƒˆë²½** | ì €ë…ì— ê¸‰ê²©íˆ ëƒ‰ê°ë˜ëŠ” ì‹œì ê³¼ ê³ ê¸°ì••ì— ì˜í•œ ëŒ€ê¸° ì •ì²´ê°€ ê²¹ì³, ê°€ì„ ì´ˆÂ·ì¤‘ê¸°ì— ì•ˆê°œÂ·ì˜¨ë„ ì—­ì „ í˜„ìƒì´ ê°€ì¥ ë¹ˆë²ˆí•©ë‹ˆë‹¤. |\n",
      "\n",
      "> **í•µì‹¬ ì •ë¦¬**: ê°€ì„ì€ **ë°¤ì˜ ë³µì‚¬ ëƒ‰ê°ê³¼ ê³ ê¸°ì••ì— ì˜í•œ ëŒ€ê¸° ì •ì²´**ê°€ ë™ì‹œì— ì‘ìš©í•´ ì•ˆê°œì™€ ì˜¨ë„ ì—­ì „ í˜„ìƒì´ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ì‹œê¸°ì…ë‹ˆë‹¤. ì´ëŠ” êµí†µÂ·í•­ê³µ ì•ˆì „ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ“Œ ê°€ì„ì— ë‚˜íƒ€ë‚˜ëŠ” í˜„ìƒë“¤ì„ í•œëˆˆì— ì •ë¦¬í•˜ë©´\n",
      "\n",
      "| í˜„ìƒ | ì£¼ëœ ì›ì¸ | ëŒ€í‘œì ì¸ ì˜í–¥/ì¬í•´ |\n",
      "|------|-----------|-------------------|\n",
      "| **íƒœí’** | ë”°ëœ»í•œ í•´ìˆ˜ë©´ + ë™ìª½ ì œíŠ¸ê¸°ë¥˜ | ê°•í’Â·í­ìš°Â·í•´ì¼ |\n",
      "| **ì¤‘ìœ„ë„ ì €ê¸°ì••Â·ì „ì„ ì„± í­í’** | ì‹œë² ë¦¬ì•„ ê³ ê¸°ì•• ë‚¨í•˜ + í•´ì–‘ì„± ì €ê¸°ì•• | ê¸‰ê²©í•œ ê¸°ì˜¨ ê°•í•˜Â·í­ìš°Â·ëˆˆÂ·ëŒí’ |\n",
      "| **ì•ˆê°œÂ·ì˜¨ë„ ì—­ì „** | ë³µì‚¬ ëƒ‰ê° + ê³ ê¸°ì•• ì •ì²´ | ì‹œì • ì €í•˜Â·êµí†µÂ·í•­ê³µ ìœ„í—˜ |\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸ™‹â€â™€ï¸ ìš”ì•½\n",
      "- **ê°€ì„ì— ê°€ì¥ ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ â€œíƒœí’â€**ì´ ë§ì§€ë§Œ, **ì¤‘ìœ„ë„ ì €ê¸°ì••Â·ì „ì„ ì„± í­í’**ê³¼ **ì•ˆê°œÂ·ì˜¨ë„ ì—­ì „**ë„ ê°€ì„ì— ë§¤ìš° í”íˆ ë‚˜íƒ€ë‚˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤.\n",
      "- ì´ ì„¸ í˜„ìƒì€ ëª¨ë‘ **ëŒ€ê¸°Â·í•´ì–‘Â·ì§€í‘œë©´ì´ ê¸‰ê²©íˆ ì „í™˜ë˜ëŠ” ê°€ì„**ì— ë°œìƒ ë©”ì»¤ë‹ˆì¦˜ì´ ê²¹ì³ì„œ ë¹ˆë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.\n",
      "- ê°ê°ì€ **ê¸°í›„Â·ì¬í•´ ê´€ë¦¬** ì¸¡ë©´ì—ì„œ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ë¯€ë¡œ, ê°€ì„ì²  ê¸°ìƒì˜ˆë³´ì™€ ëŒ€ë¹„ê°€ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "\n",
      "ê¶ê¸ˆí•œ ì ì´ ë” ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸŒ°ğŸğŸŒ¦ï¸\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë¶€ë¶„ ë³€ìˆ˜ ì ìš©)\n",
    "prompt = PromptTemplate(\n",
    "    \n",
    "    template=\"{season}ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ {phenomenon}ì´ ë§ë‚˜ìš”? {season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ì§€êµ¬ê³¼í•™ í˜„ìƒì„ 3ê°œ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
    "    input_variables=[\"phenomenon\"],  # ì‚¬ìš©ì ì…ë ¥ í•„ìš”\n",
    "    partial_variables={\"season\": get_current_season()}  # ë™ì ìœ¼ë¡œ ê³„ì ˆ ê°’ í• ë‹¹\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# íŠ¹ì • ê³„ì ˆì˜ í˜„ìƒ ì§ˆì˜\n",
    "query = prompt.format(phenomenon=\"íƒœí’ ë°œìƒ\")\n",
    "result = llm.invoke(query)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\" í”„ë¡¬í”„íŠ¸: {query}\")\n",
    "print(f\" ëª¨ë¸ ì‘ë‹µ: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ê³„ì ˆ: ê°€ì„\n",
      "\n",
      " ê°€ì„ì— ë°œìƒí•˜ëŠ” ìì—° í˜„ìƒ:\n",
      "ê°€ì„ì— ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€ë¥¼ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1.  **ì›”ì‹**: ì§€êµ¬ê°€ íƒœì–‘ê³¼ ë‹¬ ì‚¬ì´ë¥¼ ì§€ë‚˜ë©´ì„œ ë‹¬ì— ê·¸ë¦¼ìë¥¼ ë“œë¦¬ìš°ëŠ” í˜„ìƒì…ë‹ˆë‹¤. íƒœì–‘, ì§€êµ¬, ë‹¬ì´ ì¼ì§ì„ ìƒì— ìœ„ì¹˜í•  ë•Œ ë°œìƒí•˜ë©°, íƒœì–‘ì˜ ë¹›ì´ ë‹¬ì— ë‹¿ì§€ ëª»í•˜ê²Œ ë˜ì–´ ë‹¬ì— ê·¸ë¦¼ìê°€ ë°œìƒí•©ë‹ˆë‹¤. ì›”ì‹ì€ ì§€êµ¬ê³¼í•™ì ìœ¼ë¡œ ì¤‘ë ¥ì˜ ì˜í–¥ì„ ë°›ìœ¼ë©°, ë‹¬ì˜ ê¶¤ë„ê°€ ì§€êµ¬ì˜ ê¶¤ë„ì™€ ì¼ì¹˜í•˜ì§€ ì•Šì•„ì„œ ë°œìƒí•©ë‹ˆë‹¤.\n",
      "2.  **ì„±ìš´**: ë³„ì—ì„œ ë¿œì–´ì ¸ ë‚˜ì˜¤ëŠ” ê°€ìŠ¤ë‚˜ ë¨¼ì§€ê°€ ëª¨ì—¬ í˜•ì„±ëœ êµ¬ë¦„ ëª¨ì–‘ì˜ ì²œì²´ë¥¼ ë§í•©ë‹ˆë‹¤. ì„±ìš´ì€ ë³„ì˜ ìƒì„±ê³¼ ì†Œë©¸ ê³¼ì •ì—ì„œ ë°œìƒí•˜ë©°, ë³„ì˜ ìƒëª…ì„ ì—°ì¥í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì„±ìš´ì€ ì£¼ë¡œ ê°€ì„ì— ë°œìƒí•˜ë©°, ì„±ìš´ì˜ ì¢…ë¥˜ë¡œëŠ” ì„±ê°„ ë¬¼ì§ˆ, ë³„ì˜ ì”í•´, ê°€ìŠ¤ êµ¬ë¦„ ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "3.  **íƒœì–‘ì˜ í™©í˜¼**: íƒœì–‘ì˜ ì²œì •ì— ë¹„ê°€ ë‚´ë¦¬ë©´ì„œ íƒœì–‘ì´ ì§€ëŠ” í•˜ëŠ˜ì— ë°œìƒí•˜ëŠ” ë¶‰ì€ìƒ‰ì˜ êµ¬ë¦„ í˜„ìƒì…ë‹ˆë‹¤. íƒœì–‘ì˜ í™©í˜¼ì€ ëŒ€ê¸° ì¤‘ì˜ ë¬¼ë°©ìš¸ì´ë‚˜ ì–¼ìŒ ê²°ì •ì— ì˜í•´ ë°œìƒí•˜ë©°, íƒœì–‘ì˜ ë¹›ì´ ì‚°ë€ë˜ì–´ ë¶‰ì€ìƒ‰ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. íƒœì–‘ì˜ í™©í˜¼ì€ ê°€ì„ì— ë°œìƒí•˜ëŠ” í˜„ìƒìœ¼ë¡œ, ëŒ€ê¸° ì¤‘ì˜ ìˆ˜ë¶„ í•¨ëŸ‰ì´ ë†’ì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ í˜„ìƒë“¤ì€ ì§€êµ¬ê³¼í•™ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ë©°, ìì—°ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê²½í—˜í•  ìˆ˜ ìˆëŠ” ê¸°íšŒì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# Step 1: í˜„ì¬ ê³„ì ˆ ê²°ì •\n",
    "season = get_current_season(\"north\")  # ê³„ì ˆ ê°’ ì–»ê¸°\n",
    "print(f\"í˜„ì¬ ê³„ì ˆ: {season}\")\n",
    "\n",
    "# Step 2: í•´ë‹¹ ê³„ì ˆì˜ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. \"\n",
    "    \"ê° í˜„ìƒì— ëŒ€í•´ ê°„ë‹¨í•œ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# llm = ChatOpenAI(\n",
    "#     #api_key=OPENAI_API_KEY,\n",
    "#     base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "#     model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "#     temperature=0.0\n",
    "# )\n",
    "\n",
    "# ì²´ì¸ 2: ìì—° í˜„ìƒ ì¶”ì²œ (ì…ë ¥: ê³„ì ˆ â†’ ì¶œë ¥: ìì—° í˜„ìƒ ëª©ë¡)\n",
    "chain2 = (\n",
    "    {\"season\": lambda x : season}  # chain1ì˜ ì¶œë ¥ì„ season ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì‹¤í–‰: í˜„ì¬ ê³„ì ˆì— ë”°ë¥¸ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "response = chain2.invoke({})\n",
    "print(f\"\\n {season}ì— ë°œìƒí•˜ëŠ” ìì—° í˜„ìƒ:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2) PartialPrompt \n",
    "* API í˜¸ì¶œ ë°ì´í„°, ì‹œê°„ ì •ë³´, ì‚¬ìš©ì ì •ë³´ ë“±ì„ ë°˜ì˜í•  ë•Œ ë§¤ìš° ìœ ìš©í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì‹¤ì‹œê°„ í™˜ìœ¨ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1ë‹¬ëŸ¬ = {data['rates']['KRW']}ì›\"\n",
    "\n",
    "# Partial Prompt í™œìš©\n",
    "prompt = PromptTemplate(\n",
    "    template=\"í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\",\n",
    "    input_variables=[],  # ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ìë™ ë°˜ì˜\n",
    ")\n",
    "\n",
    "# LLM ëª¨ë¸ ì„¤ì •\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°\n",
    "response = llm.invoke(prompt.format())\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\" í”„ë¡¬í”„íŠ¸:\", prompt.format())\n",
    "print(\" ëª¨ë¸ ì‘ë‹µ:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
