{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry install -q langchain\n",
    "!poetry install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬 Ollama로 설치한 gemma2 모델을 사용하기\n",
    "##### ollama run gemma2\n",
    "\n",
    "- `ChatOllama` 를 활용한 LLM 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain은 대규모 언어 모델(LLM)을 활용하여 다양한 애플리케이션을 구축할 수 있도록 돕는 프레임워크입니다. 간단히 말해, LLM을 더 똑똑하고 유용하게 사용하는 도구 상자라고 생각할 수 있습니다. \\n\\n**LangChain의 주요 특징 및 기능:**\\n\\n* **LLM 연결:** 다양한 LLM(GPT-3, GPT-4, Claude 등)과 쉽게 연결될 수 있도록 제공합니다. 어떤 LLM을 사용하든 LangChain을 통해 통합하여 사용할 수 있습니다.\\n* **체인(Chains) 구축:** LLM을 단순히 텍스트를 생성하는 데 사용하는 것을 넘어, 여러 단계를 연결하여 복잡한 작업을 수행할 수 있도록 도와줍니다. 예를 들어, 질문 답변 시스템, 문서 요약기, 챗봇 등을 체인 형태로 만들 수 있습니다.\\n* **데이터 연결:** LLM이 외부 데이터 소스(웹 페이지, 데이터베이스, 문서 등)에 접근하고 활용할 수 있도록 지원합니다. 이를 통해 LLM은 더욱 정확하고 맥락에 맞는 답변을 생성할 수 있습니다.\\n* **메모리:** 대화의 맥락을 유지하고 이전 대화 내용을 기억할 수 있도록 지원합니다. 이를 통해 더욱 자연스럽고 상호작용적인 챗봇을 만들 수 있습니다.\\n* **에이전트(Agents):** LLM을 도구와 함께 사용하여 자동화된 작업을 수행할 수 있도록 합니다. 예를 들어, 웹 검색을 통해 정보를 가져오거나, 다른 API를 호출하여 작업을 수행할 수 있습니다.\\n\\n**LangChain의 활용 분야:**\\n\\n* **챗봇:** 질문 답변, 대화형 서비스, 고객 지원 등 다양한 챗봇 개발에 활용됩니다.\\n* **문서 요약:** 긴 문서를 자동으로 요약하여 핵심 내용을 빠르게 파악할 수 있도록 돕습니다.\\n* **데이터 분석:** 데이터를 분석하고 인사이트를 도출하는 데 활용됩니다.\\n* **콘텐츠 생성:** 블로그 게시물, 기사, 소셜 미디어 콘텐츠 등 다양한 유형의 콘텐츠를 생성하는 데 활용됩니다.\\n* **교육:** 학생들을 위한 개인 맞춤형 학습 경험을 제공하는 데 활용됩니다.\\n\\n**LangChain의 장점:**\\n\\n* **개발 편의성:** LLM을 활용한 애플리케이션을 쉽고 빠르게 개발할 수 있도록 다양한 도구와 기능을 제공합니다.\\n* **유연성:** 다양한 LLM과 데이터 소스에 대한 지원을 통해 다양한 요구사항에 맞는 애플리케이션을 구축할 수 있습니다.\\n* **활발한 커뮤니티:** 활발한 개발자 커뮤니티를 통해 기술 지원과 정보 공유가 활발하게 이루어지고 있습니다.\\n\\n**더 자세한 정보는 다음 링크에서 확인할 수 있습니다:**\\n\\n* **LangChain 공식 홈페이지:** [https://www.langchain.com/](https://www.langchain.com/)\\n* **LangChain 공식 문서:** [https://python.langchain.com/](https://python.langchain.com/)\\n\\n궁금한 점이 있다면 언제든지 다시 질문해주세요.' additional_kwargs={} response_metadata={'model': 'gemma3', 'created_at': '2025-09-19T06:33:07.4577297Z', 'done': True, 'done_reason': 'stop', 'total_duration': 28773382500, 'load_duration': 187617700, 'prompt_eval_count': 16, 'prompt_eval_duration': 506902900, 'eval_count': 661, 'eval_duration': 28073183500, 'model_name': 'gemma3'} id='run--44499a7d-0a2a-4ed5-827f-0449281b6fd5-0' usage_metadata={'input_tokens': 16, 'output_tokens': 661, 'total_tokens': 677}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3\")\n",
    "response = llm.invoke(\"LangChain은 무엇인가요?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬 Ollama로 설치한 llama3.2 모델을 사용하기\n",
    "##### ollama run llama3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델을 로드\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Q: {question}\\nA:\"\n",
    ")\n",
    "\n",
    "# LLMChain 생성\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# 질문을 입력하고 모델의 응답을 받음\n",
    "question = \"What is LangChain?\"\n",
    "#question = \"France의 수도는 어디입니까?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 최신버전 LangChain에서는 ChatOllama와 RunnableSequence(prompt | llm) 를 사용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"Okay, let's break down what LangChain is. It’s a powerful and increasingly popular framework designed to simplify the development of applications powered by large language models (LLMs) like GPT-3, GPT-4, PaLM, and others. Here’s a detailed explanation:\\n\\n**Core Idea:**\\n\\nLangChain essentially provides a standardized set of components and abstractions that make it *much* easier to build sophisticated applications that leverage LLMs.  Traditionally, working with LLMs was a complex, low-level process involving a lot of manual coding for things like:\\n\\n*   **Prompt Engineering:** Crafting the perfect prompts to get the LLM to do what you want.\\n*   **Data Loading & Management:**  Getting the data you need into the LLM’s context.\\n*   **Chaining Together LLM Calls:**  Combining multiple LLM calls to perform more complex tasks.\\n*   **Memory Management:**  Giving the LLM access to previous interactions to maintain context and “remember” things.\\n\\nLangChain handles much of this behind the scenes, allowing developers to focus on the *logic* of their application rather than the intricate details of interacting with the LLM.\\n\\n**Key Components and Concepts:**\\n\\n*   **Chains:** The fundamental building block. A Chain is a sequence of calls to LLMs or other utilities. These chains can be simple (e.g., prompt an LLM and print the response) or incredibly complex, involving multiple steps like retrieving information, transforming it, and then feeding it back to the LLM.\\n\\n*   **Models:**  LangChain provides abstractions for different types of LLMs. It doesn't dictate *which* LLM you use; it provides a consistent interface, making it easy to swap between models.\\n\\n*   **Prompts:** LangChain offers tools for managing and optimizing prompts.  It can help you build reusable prompt templates,  handle prompt variables, and even optimize prompts for specific LLMs.\\n\\n*   **Indexes & Retrieval:** This is a *huge* part of LangChain. It provides ways to connect your LLM to your *own* data.  This is crucial for applications like:\\n    *   **Retrieval-Augmented Generation (RAG):**  Instead of relying solely on the LLM's pre-trained knowledge, you can provide it with relevant documents or data to base its responses on. LangChain provides tools for indexing data (making it searchable), retrieving relevant chunks of data, and feeding that data into the LLM's prompt.\\n\\n*   **Memory:** LangChain offers various memory modules to allow LLM applications to maintain context over multiple turns of a conversation. There are different types of memory:\\n    *   **ConversationBufferMemory:**  Simply stores the entire conversation history.\\n    *   **ConversationSummaryMemory:**  Summarizes the conversation to reduce the context length.\\n    *   **VectorstoreMemory:** Uses embeddings to search for relevant chunks of information.\\n\\n*   **Agents:**  Agents are systems that use an LLM to decide which actions to take. They can combine LLM calls with tools (like search engines, calculators, or databases) to accomplish a task. Think of an agent as an LLM-powered decision-maker.\\n\\n**Why is LangChain Popular?**\\n\\n*   **Reduced Complexity:** It significantly simplifies the development process.\\n*   **Rapid Prototyping:**  You can quickly build and experiment with LLM-powered applications.\\n*   **Modularity:**  Its components are designed to be reusable and interchangeable.\\n*   **Large Community:**  A thriving community provides support, examples, and integrations.\\n\\n**Use Cases:**\\n\\n*   **Chatbots:**  More sophisticated than simple rule-based chatbots.\\n*   **Question Answering:**  Answering questions based on specific documents or data.\\n*   **Data Analysis:**  Extracting insights from data.\\n*   **Content Generation:** Creating different types of content.\\n*   **Agents that interact with tools:** Automating complex workflows.\\n\\n**Resources to Learn More:**\\n\\n*   **Official LangChain Website:** [https://www.langchain.com/](https://www.langchain.com/)\\n*   **LangChain Documentation:** [https://python.langchain.com/](https://python.langchain.com/)\\n*   **LangChain Examples:** [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)\\n\\n\\nDo you want me to delve deeper into a specific aspect of LangChain, such as:\\n\\n*   A particular component (e.g., Agents, Memory)?\\n*   A specific use case (e.g., building a chatbot)?\\n*   How LangChain compares to other LLM frameworks?\" additional_kwargs={} response_metadata={'model': 'gemma3', 'created_at': '2025-09-19T06:34:21.826847Z', 'done': True, 'done_reason': 'stop', 'total_duration': 43144794800, 'load_duration': 216796200, 'prompt_eval_count': 36, 'prompt_eval_duration': 139673200, 'eval_count': 1008, 'eval_duration': 42751189500, 'model_name': 'gemma3'} id='run--d1b18274-2c38-496e-9aa6-6a87b7bcf19f-0' usage_metadata={'input_tokens': 36, 'output_tokens': 1008, 'total_tokens': 1044}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "#from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"gemma3\")\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "# prompt_template = PromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
